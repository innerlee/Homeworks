\section{Stochastic Bandits, Upper Bound} % 1

References: You can find the algorithms UCB1 and UCB2 in \cite{Auer2002}.
A good reference on the bandit problem is \cite{MAL024}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Upper Bound of UCB1} %1.1

\begin{alg}[UCB1] \leavevmode
    \begin{framed}
        \begin{algorithmic}
            \For{$t=1,\dots,K$}
                \State Play arm $t$
            \EndFor
            \For{$t=K+1,\dots,T$}
                \State Play arm $i\in\argmax \bar x_{i}(t-1) + \sqrt{\frac{2\ln t}{T_i(t-1)}}$
            \EndFor
        \end{algorithmic}
    \end{framed}
\end{alg}

\textbf{Remark}
Here,
the term $\bar x_{i}(t-1)$ is \emph{exploitation},
$\sqrt{\frac{2\ln t}{T_i(t-1)}}$ is \emph{exploration},
and $+$ means \emph{optimistic}.

\textbf{Notation}
\begin{itemize}
    \item $K$ arms.
        Arm $i$ has fixed, unknown reward distribution $p_i$,
        with expectation $\mu_i$.
        Assume the support of $p_i$ is a subset of $[0,1]$, and
        also assume that not all $\mu_i$ are equal to each other.
    \item $\bar X_{i}(t)$ be the sample mean of rewards from arm $i$ during first $t$ plays of the game.
    \item $\bar X_{i,s}$ be the sample mean of rewards from the first $s$ plays of arm $i$.
    \item $\mu_*:=\max_{i\in[k]}\mu_i$, where $[k]:=\{1,\dots,k\}$.
    \item $\Delta_i:=\mu_* - \mu_i$,
        $\Delta_{\text{min}}:=\min_{i:\Delta_i>0}\Delta_i$,
        $\Delta_{\text{max}}:=\max_i \Delta_i$.
    \item $I_t$ is the index of arm that was chosen at time $t$.
    \item $T_i(t)$ is the number of times the $i$-th arm was played during first $t$ plays,
        and let $T_i:=T_i(T)$.
    \item Pseudo-regret
        \begin{equation}
            \Rbar:=T\mu_*-\sum_{t=1}^T E[\mu_{I_t}]
            = \sum_{i:\Delta_i>0} E[T_i]\cdot\Delta_i.
        \end{equation}
        The sample space consists of experiments of \emph{infinite} plays.
\end{itemize}
\

\begin{lem}
    In UCB1, if $T=\infty$, then each arm is played infinite times.
\end{lem}
\begin{proof}
    Otherwise, suppose arm $i$ was played finite times.
    Then $\sqrt{\frac{2\ln t}{T_i(t-1)}} \to \infty$.
    There must be one arm $j$, \st $T_j(t)\ge\ln t$
    (If not, $K\ln t < t$ when $t$ is large, a contradiction).
    Then $\sqrt{\frac{2\ln t}{T_j(t-1)}} \le C$ for some constant when $t$ is large.
    So, after a long time, there is no reason for playing arm $j$
    rather than playing arm $i$.
\end{proof}

So, random variables $\bar X_{i,s}$ are well defined for all
$i=1,\dots,K$ and $s=1,2,\dots$.\\


\begin{thm}
    UCB1 has pseudo-regret
    \begin{equation}
        \Rbar \le \cO\bigg(\sum_{i:\Delta_i>0} \frac{\ln T}{\Delta_i}+\Delta_i\bigg)
            \le \cO\bigg(\frac{K\ln T}{\Delta_\text{min}}+K\Delta_\text{max}\bigg). \label{eq:1_thm}
    \end{equation}
\end{thm}
\begin{proof}
    We show that for all sub-optimal arm $i$,
    \begin{equation}
        E[T_i] \le \frac{8\ln T}{\Delta_i^2} + 1 + \frac{\pi^2}{3}.
    \end{equation}
    Note that
    \begin{align}
        E[T_i]
            &= \sum_{t=1}^T \Pr[I_t=i] \\
            &\le l + \sum_{t=l+1}^T \Pr[I_t=i, T_i(t-1)\ge l] \\
            &\le l + \sum_{t=l+1}^T \Pr\Bigg[\bar X_{i}(t-1) +
                \sqrt{\frac{2\ln t}{T_i(t-1)}}\ge \bar X_{*}(t-1)+
                \sqrt{\frac{2\ln t}{T_{*}(t-1)}}, T_i(t-1)\ge l\Bigg] \label{eq:1_1_1}\\
            &\le l + \sum_{t=l+1}^T \sum_{s_i=l}^{t-1} \sum_{s=1}^{t-1}
                \Pr\Bigg[\bar X_{i,s_i} + \sqrt{\frac{2\ln t}{s_i}}\ge
                \bar X_{*,s}+\sqrt{\frac{2\ln t}{s}}\Bigg]. \label{eq:1_1_2}
    \end{align}
    From \eqref{eq:1_1_1} to \eqref{eq:1_1_2} is because the event
    \begin{equation}
        \Bigg\{\bar X_{i}(t-1) + \sqrt{\frac{2\ln t}{T_i(t-1)}}\ge
        \bar X_{*}(t-1)+\sqrt{\frac{2\ln t}{T_{*}(t-1)}}, T_i(t-1)\ge l\Bigg\}
    \end{equation}
    is a subset of
    \begin{equation}
        \cup_{s_i=l}^{t-1} \cup_{s=1}^{t-1}\Bigg\{\bar X_{i,s_i} + \sqrt{\frac{2\ln t}{s_i}}\ge
        \bar X_{*,s}+\sqrt{\frac{2\ln t}{s}}\Bigg\}.
    \end{equation}
    Observe that $\bar X_{i,s_i} + \sqrt{\frac{2\ln t}{s_i}}\ge\bar X_{*,s}+\sqrt{\frac{2\ln t}{s}}$
    fails when the following three conditions hold,
    \begin{align}
        \bar X_{i,s_i} + \sqrt{\frac{2\ln t}{s_i}} &< \mu_i + 2\sqrt{\frac{2\ln t}{s_i}}, \label{eq:1_2} \\
        \bar X_{*,s} + \sqrt{\frac{2\ln t}{s}} &> \mu_*,  \\
        \mu_* &\ge \mu_i + 2\sqrt{\frac{2\ln t}{s_i}}. \label{eq:1_3}
    \end{align}
    Note that if $l$ is large enough,
    say $l=\lceil (8\ln T)/\Delta_i^2\rceil$,
    the third inequality will hold.
    In this case,
    \eqref{eq:1_1_2} is possibly true only if we either break \eqref{eq:1_2} or break \eqref{eq:1_3}.
    We use the Chernoff-Hoeffding bound.
    \begin{framed}
        \begin{fact}[Chernoff-Hoeffding bound]
            Let $X_1, \dots, X_n$ be independent random variables with common range $[0,1]$.
            The empirical mean is $\bar X=\frac{1}{n}(X_1+\cdots+ X_n)$.
            Then for all $a\ge0$,
            \begin{equation}
                \Pr[\bar X\ge E[\bar X] + a] \le e^{-2na^2}, \quad
                \Pr[\bar X\le E[\bar X] - a] \le e^{-2na^2}.
            \end{equation}
        \end{fact}
    \end{framed}
    So,
    \begin{align}
        \Pr\Bigg[~\bar X_{i,s_i} \ge \mu_i + \sqrt{\frac{2\ln t}{s_i}}~\Bigg]
            &\le e^{-4\ln t} = t^{-4} \\
        \Pr\Bigg[~\bar X_{*,s} \le \mu_* - \sqrt{\frac{2\ln t}{s}}~\Bigg]
            &\le e^{-4\ln t} = t^{-4}.
    \end{align}
    Then,
    \begin{align}
        E[T_i]
            &\le \bigg\lceil \frac{8\ln T}{\Delta_i^2}\bigg\rceil +
            \sum_{t=\lceil (8\ln T)/\Delta_i^2\rceil+1}^T
            \sum_{s_i=l}^{t-1} \sum_{s=1}^{t-1}
            2t^{-4} \\
            &\le \frac{8\ln T}{\Delta_i^2} + 1 +
            2\sum_{t=1}^\infty \sum_{s_i=1}^{t} \sum_{s=1}^{t}t^{-4} \\
            &= \frac{8\ln T}{\Delta_i^2} + 1 + \frac{\pi^2}{3}.
    \end{align}
\end{proof}

\textbf{Remark}
We have linear dependence on $K$,
and log dependence on $T$.
