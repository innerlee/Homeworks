\section{Note 1}

References: You can find the algorithms UCB1 and UCB2 in \cite{Auer2002}.
A good reference on the bandit problem is \cite{MAL024}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Upper Bound of UCB1} %1.1

\begin{alg}[UCB1] \leavevmode
    \begin{framed}
        \begin{algorithmic}
            \For{$t=1,\dots,K$}
                \State Play arm $t$
            \EndFor
            \For{$t=K+1,\dots,T$}
                \State Play arm $i\in\argmax \bar x_i + \sqrt{\frac{2\ln t}{T_i(t)}}$
            \EndFor
        \end{algorithmic}
    \end{framed}
\end{alg}

\textbf{Remark}
Here,
the term $\bar x_i$ is \emph{exploitation},
$\sqrt{\frac{2\ln t}{T_i(t)}}$ is \emph{exploration},
and $+$ means \emph{optimistic}.

\textbf{Notation}
\begin{itemize}
    \item $K$ arms.
        Arm $i$ has fixed, unknown reward distribution $p_i$,
        with expectation $\mu_i$.
        Assume the support of $p_i$ is a subset of $[0,1]$, and
        also assume that not all $\mu_i$ are equal to each other.
    \item At time $t$, playing arm $i$ yields reward $X_{i,t}\sim p_i$.
    \item $\mu_*:=\max_{i\in[k]}\mu_i$, where $[k]:=\{1,\dots,k\}$.
    \item $\Delta_i:=\mu_* - \mu_i$,
        $\Delta_{\text{min}}:=\min_{i:\Delta_i>0}\Delta_i$,
        $\Delta_{\text{max}}:=\max_i \Delta_i$.
    \item $I_t$ is the index of arm that was chosen at time $t$.
    \item $T_i(t)$ is the number of times the $i$-th arm was played by time $t$,
        and let $T_i:=T_i(T)$.
    \item Pseudo-regret
        \begin{equation}
            R:=T\mu_*-\sum_{t=1}^T E[\mu_{I_t}]
            = \sum_{i:\Delta_i>0} E[T_i]\cdot\Delta_i.
        \end{equation}
\end{itemize}

\begin{thm}
    UCB1 has pseudo-regret
    \begin{equation}
        R \le \cO\bigg(\sum_{i:\Delta_i>0} \frac{\ln T}{\Delta_i}+\Delta_i\bigg)
            \le \cO\bigg(\frac{K\ln T}{\Delta_\text{min}}+K\Delta_\text{max}\bigg).
    \end{equation}
\end{thm}
\begin{proof}
    We show that for all sub-optimal arm $i$,
    \begin{equation}
        E[T_i] \le \frac{8\ln T}{\Delta_i^2} + \frac{\pi^2}{9}.
    \end{equation}

    \begin{align}
        E[T_i]
            &= \sum_{t=1}^T \Pr[I_t=i] \\
            &\le l + \sum_{t=l+1}^T \Pr[I_t=i, T_i>l] \\
            &\le l + \sum_{t=l+1}^T \Pr\bigg[\bar X_i + \sqrt{\frac{2\ln t}{T_i(t)}}\ge \bar X_{i*}+\sqrt{\frac{2\ln t}{T_{i*}(t)}}\bigg|T_i>l\bigg]
    \end{align}
\end{proof}

\textbf{Remark}
We have linear dependence on $K$,
and log dependence on $T$.
