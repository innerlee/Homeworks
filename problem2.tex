\section{Problem 2}

First we claim that the nuclear norm function $\|X\|_*$ is convex,
and treat it as a known fact.
Suppose $X=USV^T$ is an SVD of $X$,
then $U\in\cO(m), V\in\cO(n)$ are two orthogonal matrices.
\begin{equation}\label{eq:reduce}
\begin{split}
    \partial \|X\|_*
        &=\{D:\|X+\Delta\|_*-\|X\|_*\ge D\cdot\Delta, \forall\Delta\in\real^{m\times n}\} \\
        &=\{D:\|U^T(X+\Delta)V\|_*-\|U^TXV\|_*\ge (U^TDV)\cdot(U^T\Delta V), \forall\Delta\in\real^{m\times n}\} \\
        &=\{D:\|S+U^T\Delta V\|_*-\|S\|_*\ge (U^TDV)\cdot(U^T\Delta V), \forall\Delta\in\real^{m\times n}\} \\
        &=\{D:\|S+\Delta\|_*-\|S\|_*\ge (U^TDV)\cdot\Delta, \forall\Delta\in\real^{m\times n}\} \\
        &=\{UDV^T: D\in\partial \|S\|_*\}.
\end{split}
\end{equation}
So for this problem,
we only need compute $\partial\|S\|_*$,
where
\begin{equation}
    S=
    \begin{bmatrix}
        \Sigma  & 0 \\
        0       & 0 \\
    \end{bmatrix}.
\end{equation}
\begin{claim}\label{claim:1}
Suppose $f(x,y)$ is a convex function
where $x\in\real^s,y\in\real^t$.
Given point $(x_0,y_0)$,
let $g(x):=f(x,y_0)$ and $h(y):=f(x_0,y)$.
If the partial gradient $v=\nabla_x g(x_0)\in\real^s$ exists,
then
\begin{equation}
    \partial f(x_0,y_0) = \{(v,w):w\in\partial h(y_0)\}.
\end{equation}
\end{claim}
\begin{proof}
(Left $\subset$ Right)
$\forall (v,w)\in\partial f(x_0,y_0)$,
we have
\begin{equation}
    f(x_0+\delta_1,y_0+\delta_2)\ge f(x_0,y_0)+v^T\delta_1+w^T\delta_2,
        \quad\forall \delta_1\in\real^s,\delta_2\in\real^t.
\end{equation}
Let $\delta_1=0$ and $\delta_2=0$ respectively,
we will get $w\in\partial h(y_0)$ and $v=\nabla_x g(x_0)$.

(Left $\supset$ Right)
Without loss of generality,
we can assume $(x_0,y_0)=0, f(x_0,y_0)=0$ and $(v,w)=0$.
So we have $\nabla_x f(0,0)=0$ and inequalities
\begin{equation}
    f(\delta_1,0)\ge0, f(0,\delta_2)\ge 0,
        \quad \forall \delta_1\in\real^s,\delta_2\in\real^t.
\end{equation}
We need to show that $(0,0)\in\partial f(0,0)$, \ie
\begin{equation}
    f(\delta_1,\delta_2)\ge 0,
        \quad \forall \delta_1\in\real^s,\delta_2\in\real^t.
\end{equation}
If not,
then $\exists (a,b)\in\real^{s+t}$, \st $f(a,b)<0$.
For $t>0$,
we have
\begin{equation}
\begin{split}
    f\bigg(0,\frac{tb}{1+t}\bigg)
        &\le \frac{1}{1+t}f(-ta,0)+\frac{t}{1+t}f(a,b) \\
        &= t\bigg(\frac{1}{1+t}\frac{f(-ta,0)}{t}+\frac{1}{1+t}f(a,b)\bigg) \\
        &< 0 \quad(\text{for small enough } t).
\end{split}
\end{equation}
This contradicts $f(0,\delta_2)\ge0$.
\end{proof}

Next we compute the partial derivatives (if exists) by definition.
\begin{equation}
    \frac{\partial\|X\|_*}{\partial x_{ij}}\bigg|_{X=S}
        =\lim_{t\to0}\frac{\|S+tE_{ij}\|_*-\|S\|_*}{t},
\end{equation}
where $E_{ij}$ is a all zero matrix except for the $(i,j)$-th element being $1$.

Case ($i\le r,j\le r,i=j$).
For small enough $t$,
we have $\|S+tE_{ij}\|_*=\|S\|_*+t$ (trivial case).

Case ($i\le r,j\le r,i<j$).
We transform $S+tE_{ij}$ to a simpler shape.
First switch the $i$-th row and first row,
then switch the $i$-th column and first column,
then switch the $j$-th row and second row,
then switch the $j$-th column and second column.
After these operation,
the matrix becomes a block diagonal shape without changing the nuclear norm.
\begin{equation}
    \tilde X(t)=\begin{bmatrix}
    \sigma_i & t        & 0            & 0 \\
    0        & \sigma_j & 0            & 0 \\
    0        & 0        & \tilde\Sigma & 0 \\
    0        & 0        & 0            & 0 \\
\end{bmatrix},
\end{equation}
where $\tilde\Sigma$ is $\Sigma$ without its original $i,j$-th rows and columns.
So
\begin{equation}\label{eq:simple}
    \frac{\partial\|X\|_*}{\partial x_{ij}}\bigg|_{X=S}
        =\frac{\partial}{\partial t}\bigg|_{t=0}
            \bigg\|\begin{bmatrix}
                \sigma_i & t        \\
                0        & \sigma_j \\
            \end{bmatrix}\bigg\|_*.
\end{equation}
From \cite{svd},
we know that the singular values of matrix
\begin{equation}
    \begin{bmatrix}
        a & b \\
        c & d \\
    \end{bmatrix}
\end{equation}
are
\begin{equation}\label{eq:sing}
    \sigma_1=\sqrt{\frac{p+q}{2}},\quad
    \sigma_2=\sqrt{\frac{p-q}{2}},
\end{equation}
where
\begin{equation}
\begin{split}
    p &= a^2+b^2+c^2+d^2, \\
    q &= \sqrt{(a^2+b^2-c^2-d^2)^2+4(ac+bd)^2}.
\end{split}
\end{equation}
Using Equation~\eqref{eq:sing},
we can compute (long and tedious) the derivative in Equation~\eqref{eq:simple}
and get
\begin{equation}
    \frac{\partial\|X\|_*}{\partial x_{ij}}\bigg|_{X=S}
        = 0,\quad (i\le r,j\le r,i<j).
\end{equation}
Similarly,
for
case ($i\le r,j\le r,i>j$),
case ($i>r, j\le r$),
and case ($i\le r, j>r$),
we also have the partial derivatives being $0$.

However,
this method fails for case ($i>r, j>r$)
because in the corresponding Equation~\eqref{eq:simple}s,
$\sigma_i=\sigma_j=0$.
And then Equation~\eqref{eq:sing} will lead to
\begin{equation}
    \bigg\|\begin{bmatrix}
        0 & 0 \\
        t & 0 \\
    \end{bmatrix}\bigg\|_*=
    \bigg\|\begin{bmatrix}
        0 & t \\
        0 & 0 \\
    \end{bmatrix}\bigg\|_*=|t|,
\end{equation}
which are not smooth in $t$.

Let
\begin{equation}
    h(Y):=\bigg\|\begin{bmatrix}
            \Sigma & 0 \\
            0      & Y \\
        \end{bmatrix}\bigg\|_*
        =\|\Sigma\|_*+\|Y\|_*.
\end{equation}
And let
\begin{equation}
    g(Y):=\|Y\|_*,\quad Y\in\real^{(m-r)\times(n-r)}.
\end{equation}
Then $\partial h(0)=\partial g(0)$.
Apply Claim~\ref{claim:1} and Equation~\eqref{eq:reduce},
we have
\begin{equation}
    \partial \|X\|_*=\bigg\{
        S=U\begin{bmatrix}
            I_r & 0 \\
            0   & W \\
        \end{bmatrix}V^T:W\in\partial h(0)=\partial g(0)
        \bigg\}.
\end{equation}

The remaining part is to prove that
\begin{equation}
    \partial g(0)=\{W:\|W\|\le1\},
\end{equation}
where $\|W\|$ is the spectral norm, and
$\partial g(0)=\{D:\|\Delta\|_*\ge D\cdot\Delta,
\forall\Delta\in\real^{(m-r)\times(n-r)}\}$.

(Left $\subset$ Right)

(Left $\supset$ Right)
Let $W$ satisfy $\|W\|\le1$,
we want $W\cdot\Delta\le\|\Delta\|_*$ holds for any $\Delta$.
Suppose for a given $\Delta$,
it has an SVD $\Delta=U\Sigma V^T$,
where the diagonal elements of $\Sigma$ are non-negative.
Then we need prove
\begin{equation}
    (U^TWV)\cdot(U^T\Delta V)\le\|U^T\Delta V\|_*,
\end{equation}
which is equivalent to
\begin{equation}
    (U^TWV)\cdot\Sigma\le\|\Sigma\|_*.
\end{equation}
Let $t_i$ be the diagonal elements of $U^TWV$,
then the above equation is equivalent to
\begin{equation}\label{eq:toprove}
    \sum_i t_i \sigma_i \le\sum_i \sigma_i,
\end{equation}
Since $\|U^TWV\|=\|W\|\le1$,
we have $|t_i|\le1$ and thus Equation~\eqref{eq:toprove} holds.

Then we finish all the proofs.
