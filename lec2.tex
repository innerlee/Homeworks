\newpage
\section{Note 2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Independent Bound} %2.1

For UCB1, we have proved the \textbf{problem dependent bound}
\begin{equation}
    \Rbar_n\le 8 \sum_{i:\Delta_i>0} \bigg(~\frac{\ln n}{\Delta_i}+\Delta_i\bigg).
\end{equation}
There are also \textbf{problem independent bounds}.

\begin{thm}[Problem Independent Bound of UCB1]
    \begin{equation}
        \Rbar_n\le\cO(\sqrt{kn\ln n}).
    \end{equation}
\end{thm}
\begin{proof}
    We already know that
    \begin{align}
        E[T_i(n)]
            &\le \frac{8\ln n}{\Delta_i^2} + \cO(1).
    \end{align}
    Then,
    \begin{align}
        \Rbar_n
            &= \sum_{i:\Delta_i>0} \Delta_i E[T_i(n)] \\
            &\le c_1 \sum_{i:\Delta_i>0} \Delta_i \sqrt{E[T_i(n)] \frac{8\ln n}{\Delta_i^2}} \\
            &= c_1 \sum_{i:\Delta_i>0} 1\cdot \sqrt{8 E[T_i(n)] \ln n} \\
            &\le c_1 \sqrt{8 K \ln n \sum_{i:\Delta_i>0} E[T_i(n)] } \\
            &= \cO(\sqrt{kn\ln n}).
    \end{align}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lower Bound of UCB1} %2.2

For $p, q\in[0,1]$,
we use $\kl (p, q)$ to denote the Kullback-Leibler divergence between two
Bernoulli distributions with parameter $p$ and $q$ respectively. \ie,
\begin{equation}
    \kl(p,q) := p\ln\frac{p}{q} +(1-p)\ln \frac{1-p}{1-q}.
\end{equation}
The $l_1$-distance between these two distributions is
\begin{align}
    \|p-q\|_1 = |p-q|+|(1-p)-(1-q)| = 2|p-q|.
\end{align}
We have some relations between these two distances,
\begin{align}
    2(p-q)^2 \le \kl(p,q) \le \frac{(p-q)^2}{q(1-q)}.
\end{align}
The first inequality comes from Pinsker's inequality,
and the second one comes from $\ln x\le x-1$.
\begin{framed}
    \begin{fact}[Pinsker's Inequality]
        $P, Q$ are two probability distributions. Then
        \begin{align}
            l_1(P, Q) \le \sqrt{2\,\kl(P,Q)}.
        \end{align}
    \end{fact}
\end{framed}

\begin{thm}
    Suppose that we have $K$ arms,
    each with a Bernoulli reward distribution.
    For any algorithm satisfying
    \begin{align}
        E[T_i(n)] \le n^{o(1)}, \quad\forall i ~\text{with}~ \Delta_i>0
    \end{align}
    then,
    \begin{align}
        \liminf_{n\to\infty} \frac{\Rbar_n}{\ln n} \ge \sum_{i:\Delta_i>0} \frac{\Delta_i}{\kl(\mu_i,\mu_*)}.
    \end{align}
\end{thm}
Here, $E[T_i(n)] \le n^{o(1)}$ means $E[T_i(n)] \le o(n^\alpha), \forall \alpha\in(0,1)$.
We only prove the case of two arms.
\begin{proof}
    Suppose that arm 1 is optimal and arm 2 is sub-optimal, say $\mu_2<\mu_1<1$.
    We call this setting as instance 1, and
    we construct another instance with $\mu_1<\mu_2'<1$.
    Since arm 2 is optimal in instance 2,
    we have a lower bound on the number of times it is played
    (because there is an upper bound on the sub-optimal arm).
    We want to relate the arm 2 in instance 2 and the arm 2 in instance 1.

    First, we deal with the matter of change of measure.
    We use notations $P$ and $E$ if the probability and expectation are considered
    in instance 1, and $P', E'$ in instance 2.
    Let $X_{i,s}$ be the reward for the $s$-th play of arm $i$.
    Consider random variable $Y$ defined for both instances,
    \begin{equation}
        Y := \big((I_1, X_{I_1, T_{I_1}(1)}),\dots,(I_n, X_{I_n, T_{I_n}(n)})\big).
    \end{equation}
    Then
    \begin{equation}
    \begin{split}
        &P\Big(Y=\big((i_1, x_{i_1, T_{i_1}(1)}),\dots,(i_n, x_{i_n, T_{i_n}(n)})\big)\Big) \\
            &= P\Big(\big(I_1, X_{I_1, T_{I_1}(1)}\big)=\big(i_1, x_{i_1, T_{i_1}(1)}\big)\Big) \cdot \\
            &\quad~  P\Big(\big(I_2, X_{I_2, T_{I_2}(2)}\big)=\big(i_2, x_{i_2, T_{i_2}(2)}\big)~\Big|~
                \big(I_1, X_{I_1, T_{I_1}(1)}\big)=\big(i_1, x_{i_1, T_{i_1}(1)}\big)\Big) \cdots \\
            &\quad~  P\Big(\big(I_2, X_{I_2, T_{I_2}(2)}\big)=\big(i_2, x_{i_2, T_{i_2}(2)}\big)~\Big|~
                \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<n\Big) \\
    \end{split}
    \end{equation}
    Similar equation holds for $P'(Y)$.
    \begin{equation}
        \begin{split}
            &P\Big(\big(I_s, X_{I_s, T_{I_s}(s)}\big)=\big(i_s, x_{i_s, T_{i_s}(s)}\big)~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \\
            &=P\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s, I_s=i_s\Big) \\
            &=P\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~I_s=i_s\Big) \\
            &=P'\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~I_s=i_s\Big) \\
        \end{split}
    \end{equation}
    Considering that when $I_s=1$,
    \begin{align}
        P\Big(X_{I_s, T_{I_s}(s)}=x_{1, T_{1}(s)}~\Big|~I_s=1\Big) = P'\Big(X_{I_s, T_{I_s}(s)}=x_{1, T_{1}(s)}~\Big|~I_s=1\Big),
    \end{align}
    $P(Y)$ and $P(Y')$ only differs in terms when $I_s=2$.
    We have
    \begin{align}
        \frac{P'(Y)}{P(Y)}
            &= \frac{\prod_{i=1}^{T_2(n)}P'\big(X_{2, i}\big)}{\prod_{i=1}^{T_2(n)}P\big(X_{2, i}\big)} \\
            &= \frac{\mu_2'^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}
                {\mu_2^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}} \\
            &= \exp\bigg(-\ln
                \frac{\mu_2^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}
                {\mu_2'^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}\bigg) \\
            &= \exp\bigg(-\sum_{i=1}^{T_2(n)} \ln
                \frac{\mu_2^{X_{2,i}}(1-\mu_2)^{1-X_{2,i}}}
                {\mu_2'^{X_{2,i}}(1-\mu_2')^{1-X_{2,i}}}\bigg) \\
            &= \exp\bigg(-\sum_{i=1}^{T_2(n)} \ln
                \frac{\mu_2 X_{2,i} + (1-\mu_2) (1-X_{2,i})}
                {\mu_2' X_{2,i} + (1-\mu_2')(1-X_{2,i})}\bigg) \\
            &= \exp\big(-\hat\kl_{T_2(n)}\big).
    \end{align}
    The random variable $T_2(n)$ is a function of random variable $Y$.
    By changing of measure,
    we can relate $T_2(n)$ of instance 1 to that of instance 2.


    This is done by the constraint
    \begin{align}
        \kl(\mu_2, \mu_2')\le(1+\epsilon)\kl(\mu_2,\mu_1), \quad \text{for given}\; \epsilon>0.
    \end{align}
    Denote the reward for the $i$-th pulling of arm 2 as $X_{2,i}$.





    For any event $A$ in the $\sigma$-algebra generated by the random variable vector
    $(X_{2,1}, \dots, X_{2,s})$,
    we represent the probability of $A$ in instance 2 in terms of
    expectation in instance 1 by change of measure.
    \begin{align}
        P'(A)
            &= \sum_{(X_{2,1},\dots,X_{2,s})\in A}
                \mu_2'^{\sum_{i=1}^s X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^s X_{2,i}} \label{eq:2_34} \\
            &= \sum_{(X_{2,1},\dots,X_{2,s})\in A}
                \mu_2^{\sum_{i=1}^s X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^s X_{2,i}}
                \frac{\mu_2'^{\sum_{i=1}^s X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^s X_{2,i}}}
                {\mu_2^{\sum_{i=1}^s X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^s X_{2,i}}} \label{eq:2_35} \\
            &= E\Bigg[\doubleone_A \exp\bigg(-\ln
                \frac{\mu_2^{\sum_{i=1}^s X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^s X_{2,i}}}
                {\mu_2'^{\sum_{i=1}^s X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^s X_{2,i}}}\bigg)\Bigg] \\
            &= E\Bigg[\doubleone_A \exp\bigg(-\sum_{i=1}^s \ln
                \frac{\mu_2^{X_{2,i}}(1-\mu_2)^{1-X_{2,i}}}
                {\mu_2'^{X_{2,i}}(1-\mu_2')^{1-X_{2,i}}}\bigg)\Bigg] \\
            &= E\Bigg[\doubleone_A \exp\bigg(-\sum_{i=1}^s \ln
                \frac{\mu_2 X_{2,i} + (1-\mu_2) (1-X_{2,i})}
                {\mu_2' X_{2,i} + (1-\mu_2')(1-X_{2,i})}\bigg)\Bigg] \\
            &= E\big[\doubleone_A \exp(-\hat\kl_s)\big],
    \end{align}
    where
    \begin{align}
        \hat\kl_s
            &= \sum_{i=1}^s \ln
                \frac{\mu_2 X_{2,i} + (1-\mu_2) (1-X_{2,i})}
                {\mu_2' X_{2,i} + (1-\mu_2')(1-X_{2,i})} \\
            &= \bigg(\sum_{i=1}^s X_{2,i}\bigg)\ln\frac{\mu_2}{\mu_2'} +
                \bigg(s-\sum_{i=1}^s X_{2,i}\bigg)\ln\frac{1-\mu_2}{1-\mu_2'}.
    \end{align}
    Note that from \eqref{eq:2_34} to \eqref{eq:2_35},
    we have changed the the probability of event $\{X_{2,i}=1\}$ from $\mu_2'$ to $\mu_2$.
    So $\hat\kl_s$ is the (unnormalized) empirical estimate of $\kl(\mu_2,\mu_2')$.

    Let
    \begin{equation}
        f_n:=\frac{1-\epsilon}{\kl(\mu_2,\mu_2')}\ln n.
    \end{equation}
    We want $P[T_2(n)<f_n]=o(1)$.
    It is easy to guess that $P'[T_2(n)<f_n]=o(1)$.
    If we want to use change of measure to bound $P[T_2(n)<f_n]$,
    we also need to bound $\hat\kl_{T_2(n)}$.
    Specifically, we need $\hat\kl_{T_2(n)} \le \big(1-\frac{\epsilon}{2}\big)\ln n$.
    This is almost surely true for large $n$,
    because the estimate is becoming more accurate
    and being less than $\ln n$ is not hard.
    So this extra condition for $\hat\kl_{T_2(n)}$ poses no threat.

    So, step 1, the probability of the following event is $o(1)$,
    \begin{align}
        C_n=\bigg\{ T_2(n)<f_n
            ~~\text{and}~~
            \hat \kl_{T_2(n)} \le \Big(1-\frac{\epsilon}{2}\Big)\ln n \bigg\}.
    \end{align}
    First change of measure,
    \begin{align}
        P'(C_n)
            &= E\big[\doubleone_{C_n} \exp(-\hat\kl_{T_2(n)})\big] \\
            &\ge e^{-(1-\frac{\epsilon}{2})\ln n} P(C_n) \\
            &= n^{-(1-\frac{\epsilon}{2})} P(C_n).
    \end{align}
    By Markov's inequality,
    \begin{align}
        P(C_n)
            &\le n^{1-\frac{\epsilon}{2}} P'(C_n) \\
            &\le
    \end{align}

\end{proof}
