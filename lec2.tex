\newpage
\section{Note 2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Independent Bound} %2.1

For UCB1, we have proved the \textbf{problem dependent bound}
\begin{equation}
    \Rbar_n\le 8 \sum_{i:\Delta_i>0} \bigg(~\frac{\ln n}{\Delta_i}+\Delta_i\bigg).
\end{equation}
There are also \textbf{problem independent bounds}.

\begin{thm}[Problem Independent Bound of UCB1]
    \begin{equation}
        \Rbar_n\le\cO(\sqrt{kn\ln n}).
    \end{equation}
\end{thm}
\begin{proof}
    We already know that
    \begin{align}
        E[T_i(n)]
            &\le \frac{8\ln n}{\Delta_i^2} + \cO(1).
    \end{align}
    Then,
    \begin{align}
        \Rbar_n
            &= \sum_{i:\Delta_i>0} \Delta_i E[T_i(n)] \\
            &\le c_1 \sum_{i:\Delta_i>0} \Delta_i \sqrt{E[T_i(n)] \frac{8\ln n}{\Delta_i^2}} \\
            &= c_1 \sum_{i:\Delta_i>0} 1\cdot \sqrt{8 E[T_i(n)] \ln n} \\
            &\le c_1 \sqrt{8 K \ln n \sum_{i:\Delta_i>0} E[T_i(n)] } \\
            &= \cO(\sqrt{kn\ln n}).
    \end{align}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lower Bound of UCB1} %2.2

For $p, q\in[0,1]$,
we use $\kl (p, q)$ to denote the Kullback-Leibler divergence between two
Bernoulli distributions with parameter $p$ and $q$ respectively. \ie,
\begin{equation}
    \kl(p,q) := p\ln\frac{p}{q} +(1-p)\ln \frac{1-p}{1-q}.
\end{equation}
The $l_1$-distance between these two distributions is
\begin{align}
    \|p-q\|_1 = |p-q|+|(1-p)-(1-q)| = 2|p-q|.
\end{align}
We have some relations between these two distances,
\begin{align}
    2(p-q)^2 \le \kl(p,q) \le \frac{(p-q)^2}{q(1-q)}.
\end{align}
The first inequality comes from Pinsker's inequality,
and the second one comes from $\ln x\le x-1$.
\begin{framed}
    \begin{fact}[Pinsker's Inequality]
        $P, Q$ are two probability distributions. Then
        \begin{align}
            l_1(P, Q) \le \sqrt{2\,\kl(P,Q)}.
        \end{align}
    \end{fact}
\end{framed}

\begin{thm}
    Suppose that we have $K$ arms,
    each with a Bernoulli reward distribution.
    For any algorithm satisfying
    \begin{align}
        E[T_i(n)] \le n^{o(1)}, \quad\forall i ~\text{with}~ \Delta_i>0
    \end{align}
    then,
    \begin{align}
        \liminf_{n\to\infty} \frac{\Rbar_n}{\ln n} \ge \sum_{i:\Delta_i>0} \frac{\Delta_i}{\kl(\mu_i,\mu_*)}.
    \end{align}
\end{thm}
Here, $E[T_i(n)] \le n^{o(1)}$ means $E[T_i(n)] \le o(n^\alpha), \forall \alpha\in(0,1)$.
We only prove the case of two arms.
\begin{proof}

\end{proof}
