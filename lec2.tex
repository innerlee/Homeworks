\newpage
\section{Note 2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Independent Bound} %2.1

For UCB1, we have proved the \textbf{problem dependent bound}
\begin{equation}
    \Rbar_n\le 8 \sum_{i:\Delta_i>0} \bigg(~\frac{\ln n}{\Delta_i}+\Delta_i\bigg).
\end{equation}
There are also \textbf{problem independent bounds}.

\begin{thm}[Problem Independent Bound of UCB1]
    \begin{equation}
        \Rbar_n\le\cO(\sqrt{kn\ln n}).
    \end{equation}
\end{thm}
\begin{proof}
    We already know that
    \begin{align}
        E[T_i(n)]
            &\le \frac{8\ln n}{\Delta_i^2} + \cO(1).
    \end{align}
    Then,
    \begin{align}
        \Rbar_n
            &= \sum_{i:\Delta_i>0} \Delta_i E[T_i(n)] \\
            &\le c_1 \sum_{i:\Delta_i>0} \Delta_i \sqrt{E[T_i(n)] \frac{8\ln n}{\Delta_i^2}} \\
            &= c_1 \sum_{i:\Delta_i>0} 1\cdot \sqrt{8 E[T_i(n)] \ln n} \\
            &\le c_1 \sqrt{8 K \ln n \sum_{i:\Delta_i>0} E[T_i(n)] } \\
            &= \cO(\sqrt{kn\ln n}).
    \end{align}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lower Bound of UCB1} %2.2

For $p, q\in[0,1]$,
we use $\kl (p, q)$ to denote the Kullback-Leibler divergence between two
Bernoulli distributions $P$ and $Q$ with parameter $p$ and $q$ respectively. \ie,
\begin{equation}
    \kl(p,q) := \kl(P,Q) = p\ln\frac{p}{q} +(1-p)\ln \frac{1-p}{1-q}.
\end{equation}
To compare with the upper bound~\eqref{eq:1_thm}, we have
\begin{align}
    2(p-q)^2 \le \kl(p,q) \le \frac{(p-q)^2}{q(1-q)}.
\end{align}
The second inequality comes from $\ln x\le x-1$.
The first one comes from Pinsker's inequality,
being aware of that $l_1$-distance between $P$ and $Q$ is
\begin{align}
    l_1(P,Q) = |p-q|+|(1-p)-(1-q)| = 2|p-q|.
\end{align}
\begin{framed}
    \begin{fact}[Pinsker's Inequality]
        $P, Q$ are two probability distributions. Then
        \begin{align}
            l_1(P, Q) \le \sqrt{2\,\kl(P,Q)}.
        \end{align}
    \end{fact}
\end{framed}

\begin{thm}
    Suppose that we have $K$ arms,
    each with a Bernoulli reward distribution.
    For any algorithm satisfying
    \begin{align}
        E[T_i(n)] \le n^{o(1)}, \quad\forall i ~\text{with}~ \Delta_i>0
    \end{align}
    then,
    \begin{align}
        \liminf_{n\to\infty} \frac{\Rbar_n}{\ln n} \ge \sum_{i:\Delta_i>0} \frac{\Delta_i}{\kl(\mu_i,\mu_*)}.
    \end{align}
\end{thm}
Here, $E[T_i(n)] \le n^{o(1)}$ means $E[T_i(n)] \le o(n^\alpha), \forall \alpha\in(0,1)$.
We prove the case of two arms.
\begin{proof}
    Suppose that arm 1 is optimal and arm 2 is sub-optimal, say $\mu_2<\mu_1<1$.
    We call this setting as instance 1, and
    we construct another instance with $\mu_1<\mu_2'<1$.
    Since arm 2 is optimal in instance 2,
    we have a lower bound on the number of times it is played
    (because there is an upper bound on the sub-optimal arm).
    We want to relate the arm 2 in instance 2 and the arm 2 in instance 1.

    First, we deal with the matter of change of measure.
    We use notations $P$ and $E$ if the probability and expectation are considered
    in instance 1, and $P', E'$ in instance 2.
    Let $X_{i,s}$ be the reward for the $s$-th play of arm $i$.
    Consider random variable $Y$ defined for both instances,
    \begin{equation}
        Y := \big((I_1, X_{I_1, T_{I_1}(1)}),\dots,(I_n, X_{I_n, T_{I_n}(n)})\big).
    \end{equation}
    Then
    \begin{equation}
    \begin{split}
        &P\Big(Y=\big((i_1, x_{i_1, T_{i_1}(1)}),\dots,(i_n, x_{i_n, T_{i_n}(n)})\big)\Big) \\
            &= P\Big(\big(I_1, X_{I_1, T_{I_1}(1)}\big)=\big(i_1, x_{i_1, T_{i_1}(1)}\big)\Big) \cdot \\
            &\quad~  P\Big(\big(I_2, X_{I_2, T_{I_2}(2)}\big)=\big(i_2, x_{i_2, T_{i_2}(2)}\big)~\Big|~
                \big(I_1, X_{I_1, T_{I_1}(1)}\big)=\big(i_1, x_{i_1, T_{i_1}(1)}\big)\Big) \cdots \\
            &\quad~  P\Big(\big(I_2, X_{I_2, T_{I_2}(2)}\big)=\big(i_2, x_{i_2, T_{i_2}(2)}\big)~\Big|~
                \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<n\Big) \\
    \end{split}
    \end{equation}
    Similar equation holds for $P'(Y)$.
    \begin{equation}
        \begin{split}
            &P\Big(\big(I_s, X_{I_s, T_{I_s}(s)}\big)=\big(i_s, x_{i_s, T_{i_s}(s)}\big)~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \\
            &=P\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s, I_s=i_s\Big) \\
            &=P\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~I_s=i_s\Big) \\
            &=P'\Big(I_s=i_s~\Big|~
            \big(I_t, X_{I_t, T_{I_t}(t)}\big)=\big(i_t, x_{i_t, T_{i_t}(t)}\big), \forall t<s\Big) \cdot \\
            &\quad~P\Big(X_{I_s, T_{I_s}(s)}=x_{i_s, T_{i_s}(s)}~\Big|~I_s=i_s\Big) \\
        \end{split}
    \end{equation}
    Considering that when $I_s=1$,
    \begin{align}
        P\Big(X_{I_s, T_{I_s}(s)}=x_{1, T_{1}(s)}~\Big|~I_s=1\Big) = P'\Big(X_{I_s, T_{I_s}(s)}=x_{1, T_{1}(s)}~\Big|~I_s=1\Big),
    \end{align}
    $P(Y)$ and $P(Y')$ only differs in terms when $I_s=2$.
    We have
    \begin{align}
        \frac{P(Y)}{P'(Y)}
            &= \frac{\prod_{i=1}^{T_2(n)}P\big(X_{2, i}\big)}{\prod_{i=1}^{T_2(n)}P'\big(X_{2, i}\big)} \\
            &= \frac{\mu_2^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}
                {\mu_2'^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}} \\
            &= \exp\bigg(\ln
                \frac{\mu_2^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2)^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}
                {\mu_2'^{\sum_{i=1}^{T_2(n)} X_{2,i}}(1-\mu_2')^{n-\sum_{i=1}^{T_2(n)} X_{2,i}}}\bigg) \\
            &= \exp\Bigg(\sum_{i=1}^{T_2(n)} \ln
                \frac{\mu_2^{X_{2,i}}(1-\mu_2)^{1-X_{2,i}}}
                {\mu_2'^{X_{2,i}}(1-\mu_2')^{1-X_{2,i}}}\Bigg) \\
            &= \exp\Bigg(\sum_{i=1}^{T_2(n)} \ln
                \frac{\mu_2 X_{2,i} + (1-\mu_2) (1-X_{2,i})}
                {\mu_2' X_{2,i} + (1-\mu_2')(1-X_{2,i})}\Bigg) \\
            &= \exp\big(\hat\kl_{T_2(n)}\big).\label{eq:2_42}
    \end{align}
    The random variable $T_2(n)$ is a function of random variable $Y$.
    By changing of measure,
    we can relate $T_2(n)$ of instance 1 to that of instance 2.
    Specifically, we want $P[T_2(n)<f_n]=o(1)$, where
    \begin{equation}
        f_n:=\frac{1-\epsilon}{\kl(\mu_2,\mu_2')}\ln n.
    \end{equation}
    Observe Equation~\eqref{eq:2_42}, we also need $\hat\kl_{T_2(n)}$ be upper bounded,
    so that when $P'(Y)$ is small, we can infer that $P(Y)$ is also small.
    Consider event $C_n$,
    \begin{align}
        C_n=\bigg\{ T_2(n)<f_n
            ~~\text{and}~~
            \hat \kl_{T_2(n)} \le \Big(1-\frac{\epsilon}{2}\Big)\ln n \bigg\}.
    \end{align}
    We have
    \begin{align}
        P(C_n)
            &= \sum_{Y\subset C_n} P(Y) \\
            &= \sum_{Y\subset C_n} P'(Y) \exp\big(\hat\kl_{T_2(n)}\big) \\
            &\le e^{(1-\frac{\epsilon}{2})\ln n} P'(C_n) \\
            &= n^{(1-\frac{\epsilon}{2})} P'(C_n) \\
            &\le n^{(1-\frac{\epsilon}{2})} P'(T_2(n)<f_n) \\
            &= n^{(1-\frac{\epsilon}{2})} P'(T_1(n)\ge n-f_n) \\
            &\le n^{(1-\frac{\epsilon}{2})} \frac{E'[T_1(n)]}{n-f_n}.
    \end{align}
    The last inequality is due to Markov's inequality.
    Since arm 1 is sub-optimal in instance 2,
    by assumption, we have $E'[T_1(n)]\le o(n^\alpha), \forall \alpha\in(0,1)$.
    So
    \begin{equation}
        P(C_n)=o(1). \label{eq:2_52}
    \end{equation}

    Next we deal with the upper bound condition on $\hat \kl_{T_2(n)}$.
    First observe that
    \begin{align}
        P(C_n)
            &\ge P\bigg(T_2(n)<f_n~~\text{and}~~\max_{s\le f_n}\hat\kl_s\le\Big(1-\frac{\epsilon}{2}\Big)\ln n\bigg) \\
            &=  P\bigg(T_2(n)<f_n~~\text{and}~~\frac{1}{f_n}\max_{s\le f_n}\hat\kl_s\le\frac{1-\frac{\epsilon}{2}}{1-\epsilon}\kl(\mu_2,\mu_2')\bigg)\label{eq:2_54}
    \end{align}
    And also note that in instance 1,
    $\kl_s$ is the unnormalized empirical estimation of $\hat\kl(\mu_2,\mu_2')$,
    \begin{align}
        \hat\kl_s
            &= \sum_{i=1}^s \ln
                \frac{\mu_2 X_{2,i} + (1-\mu_2) (1-X_{2,i})}
                {\mu_2' X_{2,i} + (1-\mu_2')(1-X_{2,i})} \\
            &= \bigg(\sum_{i=1}^s X_{2,i}\bigg)\ln\frac{\mu_2}{\mu_2'} +
                \bigg(s-\sum_{i=1}^s X_{2,i}\bigg)\ln\frac{1-\mu_2}{1-\mu_2'}.
    \end{align}
    By the maximal version of the strong law of large numbers,
    \begin{framed}
        \begin{fact}[Maximal Version of Strong Law of Large Numbers]
            For any sequence $X_t$ of independent real random variables with positive mean $\mu>0$,
            then
            \begin{equation}
                \lim_{n\to\infty}\frac{1}{n}\sum_{t=1}^n X_t=\mu
            \end{equation}
            almost surely implies
            \begin{equation}
                \lim_{n\to\infty}\frac{1}{n}\max_{s=1,\dots,n}\sum_{t=1}^n X_t=\mu.
            \end{equation}
        \end{fact}
    \end{framed}
    \begin{align}
        \lim_{n\to\infty}P\bigg(\frac{1}{f_n}\max_{s\le f_n}\hat\kl_s<\frac{1-\frac{\epsilon}{2}}{1-\epsilon}\kl(\mu_2,\mu_2')\bigg)=1. \label{eq:2_60}
    \end{align}
    By \eqref{eq:2_52}, \eqref{eq:2_54} and \eqref{eq:2_60},
    we have
    \begin{equation}
        P(T_2(n)<f_n)=o(1).
    \end{equation}
    By Markov inequality,
    \begin{align}
        P(T_2(n)\ge f_n)
            &=1-o(1) \\
            &\le \frac{E[T_2(n)]}{f_n} \\
            &= \frac{\kl(\mu_2,\mu_2')}{(1-\epsilon)\ln n}E[T_2(n)]
    \end{align}
    If in addition, $\mu_2'$ satisfies
    \begin{align}
        \kl(\mu_2, \mu_2')\le(1+\epsilon)\kl(\mu_2,\mu_1),
    \end{align}
    we have
    \begin{align}
        E[T_2(n)]\ge(1-o(1))\frac{1-\epsilon}{1+\epsilon}\frac{\ln n}{\kl(\mu_2,\mu_1)}.
    \end{align}
\end{proof}
