\section{Geometric View of GAN} \label{sec:geom}

% def of manifold
A \emph{manifold} (without boundary) is a set of points that
locally resembles Euclidean space near each point.
Specifically,
for each point of an $n$-dimensional manifold,
there exists a neighborhood that is homeomorphic to an $n$-dimensional open
set $\Omega\subset\real^n$,
such as the open cube $\mathring{I}^n:=(0,1)^n\subset\real^n$.
Intuitively,
we can view a $n$-dimensional manifold as a \emph{surface}
in an Euclidean space.
Trivial examples are,
a \emph{point} in a line,
a \emph{circle} in a plane,
or a \emph{ball} in the $3$-dimensional space we live in.

Suppose we have an $n$-dimensional data manifold $M$ in the $N$-dimensional
($N\ge n$) Euclidean space.
For simplicity,
we assume $M$ is \emph{contractible},
\ie,
it is topologically equivalent to a point.
Choose the cube $\Omega=\mathring{I}^n$ as the latent space,
then we are able to parametrize manifold $M$ by a
single coordinate chart $(z_1,\dots,z_n)$.
Suppose the mapping from coordinate space to manifold is
\begin{equation}
    \varphi:\Omega\to M\subset \real^N,
\end{equation}
then we can view the objective of GAN as to learn such a
parametrization as $\varphi$.
There are several immediate observations on GAN from this geometric
point of view.

\paragraph{Decouple Geometry and Distribution}
In fact,
as a generative model,
being able to generate the whole set of real data points is not enough.
It should also generate samples in the right probability.
Thus,
it is natural to decouple the generation task as two subtasks:
generating the right geometry,
and the right distribution.
Following this idea,
we can first focus on generating the correct data manifold
without worrying about issues on the distribution.
For example,
we can exploit sampling tricks more freely to aid
this geometry learning process.
Once the right geometry is generated,
we can freeze the generator and attach another network
right before the latent space
to learn to adjust to the right distribution.
The latter task would be easier since
1) the dimension of latent space is lower,
and
2) the data manifold and generated manifold already have sufficient overlap.

\paragraph{Inverse Mapping}
The coordinate mapping $\varphi$ is a homeomorphism,
and thus a bijection.
The necessary and sufficient condition for $\varphi$ being a bijection is
the existence of a mapping $\psi$,
\begin{equation}
    \psi:M\to \Omega\subset\real^n,
\end{equation}
such that,
\begin{equation}\label{eq:inv}
    \psi\circ\varphi = \id_\Omega ~
    (\varphi\text{ is injective}), \quad\text{ and } \quad
    \varphi\circ\psi = \id_\cM ~
    (\varphi\text{ is surjective}).
\end{equation}
The mode collapse problem might be mitigated
provided we impose the bijection as an regularization,
because data samples are explicitly required to be generatable.
In fact,
some works~\cite{huang2016stacked,che2016mode,kim2017learning,
perarnau2016invertible,zhu2016generative}
already take the general idea of inverse mapping into consideration.
Though they mostly motivated from the auto-encoder perspective
and focus more on the reconstruction,
i.e., the second equation in~\eqref{eq:inv}.
The full power of inverse mapping is still waiting to be discovered.

\paragraph{Learning Mapping as Graph}
In conditional GAN,
we want to control the generated sample $x$ through some condition $c$.
This can be reformulated as learning a multi-valued mapping
from condition $c$ to some data sample $x$.
Geometrically,
we can represent a mapping by its graph $G:=\{(c,x)\}$.
The graph is a manifold that can be learned in the usual GAN formulation.
The idea of learning mapping as graph is a general solution to
handle the one to many relationships.
More applications other than conditional GAN are still underdeveloped.

These observations lead to some useful insights into GAN.
As we have commented,
some of the ideas have been explored recently
in one form or another.
However,
many aspects are still in mystery.

In this report,
we take a crack on the simplest yet a fundamental problem in
understanding the geometric aspects of GAN,
namely the impact of the geometric properties of
latent space and data manifold on the training of GAN.
Specifically,
we are interested in the following two properties:
\begin{itemize}
    \item \emph{Dimensionality}.
        We know that two manifolds never match
        if their dimensions are different in the first place.
        However, in real applications,
        the dimension of data manifold is unknown.
        So it is interesting to see what would happen to GAN if the dimensions
        of latent space and data manifold do not match.
    \item \emph{Connectivity}.
        Natural images are clustered into distinct classes.
        This means that the data manifold may have several disconnected
        components.
        The questions are,
        do we have to make the latent space disconnected at the same time?
        What if the latent space is disconnected while the data distribution
        is connected?
\end{itemize}
