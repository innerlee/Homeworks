\section{Note 1}
Reading~\cite{negahban2012}.
\subsection{Prelude}

\subsubsection{Recall}

Regularized loss minimization
\begin{equation}
\hat\theta \in \argmin_{\theta\in\real^d} \{\cL(\theta;\{z_i\}_{i=1}^n) + \lambda R(\theta)\}
\end{equation}
for estimating a certain unknown vector $\theta^*\in\real^d$.
Here $z_1,\dots,z_n$ are iid \wrt distribution $\Pbb_{\theta^*}$.
Denote
\begin{equation}
F := \cL(\theta;\{z_i\}_{i=1}^n) + \lambda R(\theta).
\end{equation}

\subsubsection{This Lecture}

Derive bounds on the \emph{statistical error}
$\hat\Delta = \hat\theta - \theta^*$.

\subsubsection{Motivation}

\begin{itemize}
    \item From optimization, expect
        $|F(\hat\theta) - F(\theta^*)|$
        to be small.
    \item Show that $\hat\Delta$ belongs to a region
        that $F$ is not too flat.
        This involves accounting for the effect of $R$.
    \item $R$ is intended to promote certain desirable structure (\eg sparsity).
        We need to know how $R$ penalizes deviation,
        which leads to the \emph{decomposability} concept.
\end{itemize}

\subsection{Theme}

\subsubsection{Decomposability of $R$}

\begin{itemize}
    \item $\cM$: model subspace,
        which captures the constraints specified by the model
        (\eg vectors within certain support for sparsity).
    \item Let $\cM \subset \cMbar \subset \real^d$ be a pair of subspaces.
        The \emph{perturbation subspace} is
        \begin{equation}
            \cMbar^\perp = \{v\in\real^d: u^Tv=0, \forall u\in\cMbar\}.
        \end{equation}
\end{itemize}

\begin{define}
    $R$ is decomposable \wrt $(\cM, \cMbar^\perp)$ if
    \begin{equation}
        R(\theta+\gamma) = R(\theta) + R(\gamma) \quad
        \forall \theta \in \cM, \gamma \in \cMbar^\perp.
    \end{equation}
\end{define}

\begin{obs} \leavevmode
\begin{itemize}
    \item $R$ is a norm and hence
        $R(\theta+\gamma)\le R(\theta) + R(\gamma)$.
        This means we want the perturbation term $\gamma$ be
        maximally penalized (up to the equal sign).
    \item The notion of decomposability is useful when $\cM$ is small,
        and at the same time not being too far from $\theta^*$,
        in the sense that $\Pi_\cM(\theta^*)\approx\theta^*$.
\end{itemize}
\end{obs}

\begin{ex}[Sparse vectors and $l_1$-regularization] \leavevmode
\begin{itemize}
    \item model: $S$-sparse vectors in $\real^d$, $R(\theta)=\|\theta\|_1$.
    \item for any $S\subset\{1,\dots,d\}$ of cardinality $s$,
        define $\cM(S):=\{\theta\in\real^d:\theta_j=0\ \forall j \notin S \}$.
        If $\theta^*$ is supported on $S$,
        then $\Pi_{\cM(S)}(\theta^*)=\theta^*$.
    \item take $\cMbar(S):=\cM(S)$,
        so $\cMbar(S)^\perp = \cM(S)^\perp$.
    \item decomposability is obvious.
\end{itemize}
\end{ex}

\begin{ex}[Group-structured norms] \leavevmode
\begin{itemize}
    \item motivation: groups of coefficients likely to be
        zero or non-zero simultaneously.
    \item model: partition $\{1,\dots,d\}$ into $N$ disjoint groups
        $\{G_1,\dots,G_N\}$, number of selected groups should be small,
        say $s$; \ie $S\subset \{1,\dots,N\}$
        are the group indices that correspond to non-zero groups and $s=|S|$.
        The norm is
        \begin{equation}
            R(\theta) = \|\theta\|_{g,p} = \sum_{i=1}^N\|\theta_{G_i}\|_p,\quad p \in[1,\infty].
        \end{equation}
    \item subspace: for any $S\subset\{1,\dots,N\}$, set
        \begin{gather}
            \cM(S) := \{\theta\in\real^d:\theta_{G_i}=0, \forall i\notin S\}, \\
            \cMbar(S) := \cM(S).
        \end{gather}
    \item decomposability can be verified.
\end{itemize}
\end{ex}

\subsubsection{Key Consequences of Decomposability}

\begin{pro} Suppose $\cL$ is convex, differentiable,
    $\lambda\ge 2R^*(\nabla\cL(\theta^*;\{z_i\}_{i=1}^n))$,
    where $R^*(v):=\sup_{R(u)\le 1}u^T v$.
    Then for any $(\cM, \cMbar^\perp)$ over which $R$ is decomposable,
    $\Deltahat = \thetahat - \theta^*$ belongs to
    \begin{equation}
        \cC=\{\Delta\in\real^d:R(\Delta_{\cMbar^\perp})\le 3 R(\Delta_{\cMbar}) + 4R(\theta^*_{\cM^\perp})\},
    \end{equation}
    where $\Delta_{\cMbar^\perp}=\Pi_{\cMbar^\perp}(\Delta)$.
\end{pro}

\begin{proof}
    Define
    \begin{equation}
        \cD(\Delta)=\cL(\theta^*+\Delta)-\cL(\theta^*)+\lambda(R(\theta^*+\Delta)-R(\theta^*)).
    \end{equation}
    Noting optimality of $\thetahat$, we have $\cD(\Deltahat)\le0$.

    \begin{claim}
        \begin{equation}
            R(\theta^*+\Delta)-R(\theta^*) \ge R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp}).
        \end{equation}
    \end{claim}

    \begin{proof}
        Split $\theta^*=\theta_{\cM}^*+\theta^*_{\cM^\perp}$ and $\Delta=\Delta_{\cMbar^\perp}+\Delta_{\cMbar}$,
        \begin{equation}
            \begin{split}
                R(\theta^*+\Delta) - R(\theta^*) & = R(\theta_{\cM}^*+\theta^*_{\cM^\perp}+\Delta_{\cMbar}+\Delta_{\cMbar^\perp}) - R(\theta_{\cM}^*+\theta^*_{\cM^\perp}) \\
                    & \ge (R(\theta_{\cM}^*+\Delta_{\cMbar^\perp})-R(\theta^*_{\cM^\perp}) - R(\Delta_{\cMbar})) - (R(\theta_{\cM}^*) + R(\theta_{\cM^\perp}^*))  \\
                    & = R(\Delta_{\cMbar^\perp})-2R(\theta^*_{\cM^\perp}) - R(\Delta_{\cMbar}).
            \end{split}
        \end{equation}
    \end{proof}

    \begin{claim}
        If $\lambda\ge 2R^*(\nabla\cL(\theta^*))$ and $\cL$ is convex, then
        \begin{equation}
            \cL(\theta^*+\Delta) -\cL(\theta^*) \ge -\frac{\lambda}{2} (R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp})).
        \end{equation}
    \end{claim}

    \begin{proof}
        By convexity and definition of dual norm,
        \begin{align}
            \cL(\theta^*+\Delta)-\cL(\theta^*) & \ge \nabla\cL(\theta^*)^T\Delta \\
                & \ge-|\nabla\cL(\theta^*)^T\Delta| \\
                & \ge -R^*(\nabla\cL(\theta^*))R(\Delta)    \\
                & \ge -\frac{\lambda}{2}(R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp}))
        \end{align}
    \end{proof}

    With these two claims, we have
    \begin{equation}
    \begin{split}
    0 \ge \cD(\Deltahat) & \ge \lambda[R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp})]
        -\frac{\lambda}{2}[R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp})]  \\
    & = \frac{\lambda}{2}[R(\Delta_{\cMbar^\perp})-3R(\Delta_{\cMbar})-4R(\theta^*_{\cM^\perp})].
    \end{split}
    \end{equation}
\end{proof}

Now we want $\cL$ to be not too flat in $\cC$.
To formulate this, we use \emph{Restricted Strongly Convex} (RSC).

\begin{define}
    $\cL$ satisfies RSC
    if $\exists$ constant $\kappa > 0$ and function $\tau(\cdot)$
    \st
    \begin{equation}
        \cL(\theta^*+\Delta)\ge \cL(\theta^*) + \nabla \cL(\theta^*)^T\Delta + \kappa\|\Delta\|_2^2-\tau^2(\theta^*),\quad \forall \Delta\in\cC.
    \end{equation}
\end{define}

Here comes the main theorem.

\begin{thm}
Suppose
\begin{itemize}
    \item $R$ is a decomposable norm \wrt $(\cM, \cMbar^\perp)$,
    \item $\cL$ is convex, differentiable, and RSC,
    \item $\lambda\ge 2R^*(\nabla\cL(\theta^*;\{z_i\}_{i=1}^n))$.
\end{itemize}
Then
\begin{equation}
    \|\thetahat-\theta^*\|_2^2\le 9\frac{\lambda^2}{\kappa^2}\Psi^2(\cMbar)
        + \frac{\lambda}{\kappa}\{2\tau^2(\theta^*)+4R(\theta^*_{\cM^\perp})\},
\end{equation}
where
\begin{equation}
    \Psi(\cM)=\sup_{u\in\cM\backslash\{0\}}\frac{R(u)}{\|u\|_2},
\end{equation}
is the Lipschitz constant of $R$ restricted to $\cM$.
\end{thm}

\begin{proof}
    Let $\delta>0$ be a parameter. Define
    \begin{equation}
        K(\delta) = \cC \cap \{\Delta:\|\Delta\|_2=\delta\}.
    \end{equation}

\begin{claim}
    If $\forall \Delta\in K(\delta), \cD(\Delta)>0$, then $\|\Deltahat\|_2\le\delta$.
\end{claim}

\begin{proof}

We need the following claim.

\begin{claim}
    $\cC$ is star-shaped; \ie $\forall \Delta\in\cC, \{t\Delta:t\in(0,1)\}\subset \cC$.
\end{claim}

\begin{proof}
If $\theta^*\in\cM$, $\cC$ is a cone.
Now suppose $\theta^*\notin\cM$.
Since $\cMbar$ is a subspace and projection $\Pi_{\cMbar}$ is linear,
we have
\begin{equation}
    \Pi_{\cMbar}(t\Delta) = t\Pi_{\cMbar}(\Delta), \quad
    \Pi_{\cMbar^\perp}(t\Delta) = t\Pi_{\cMbar^\perp}(\Delta).
\end{equation}
Hence for $t\in(0,1)$,
\begin{equation}
\begin{split}
    R(\Pi_{\cMbar^\perp}(t\Delta)) & = tR(\Pi_{\cMbar^\perp}(\Delta)) \\
        & \le t(3 R(\Pi_{\cMbar}(\Delta)) + 4R(\theta^*_{\cM^\perp}))    \\
        & \le 3 R(\Pi_{\cMbar}(t\Delta)) + 4R(\theta^*_{\cM^\perp}).
\end{split}
\end{equation}
\end{proof}

Suppose $\|\Deltahat\|_2>\delta$.
Then the line joining $0$ and $\Deltahat$ will intersect $K(\delta)$
at some $t^*\Deltahat$, where $t^*\in(0,1)$.
Since $\cD$ is convex and $\cD(0)=0$,
$\cD(t^*\Deltahat) \le t^*D(\Deltahat) \le 0$,
which is a contradiction.
\end{proof}

To prove the theorem,
it remains to establish a lower bound
on $\cD(\Delta)$
over $K(\delta)$ for some suitably chosen $\delta>0$.
\begin{equation}
\begin{split}
    \cD(\Delta) & = \cL(\theta^*+\Delta)-\cL(\theta^*)+\lambda(R(\theta^*+\Delta)-R(\theta^*))  \\
        & \ge \nabla\cL(\theta^*)^T\Delta + \kappa\|\Delta\|_2^2-\tau^2(\theta^*) + \lambda(R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp})) \\
        & \ge \kappa\|\Delta\|_2^2-\tau^2(\theta^*) + \lambda(R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp})) - R^*(\cL(\theta^*))R(\Delta) \\
        & \ge \kappa\|\Delta\|_2^2-\tau^2(\theta^*) + \lambda(\frac{1}{2}R(\Delta_{\cMbar^\perp})-\frac{3}{2}R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp}))   \\
        & \ge \kappa\|\Delta\|_2^2-\tau^2(\theta^*) - \frac{\lambda}{2}(3R(\Delta_{\cMbar})+4R(\theta^*_{\cM^\perp})).
\end{split}
\end{equation}
By definition of $\Psi(\cMbar)$, we have
\begin{equation}
    R(\Delta_{\cMbar}) \le \Psi(\cMbar)\|\Delta_{\cMbar}\|_2 \le \Psi(\cMbar)\|\Delta\|_2.
\end{equation}
It follows that
\begin{equation}
    \cD(\Delta) \ge \kappa\|\Delta\|_2^2-\tau^2(\theta^*) - \frac{\lambda}{2}(3\Psi(\cMbar)\|\Delta\|_2+4R(\theta^*_{\cM^\perp})).
\end{equation}
Note that the RHS is quadratic in $\|\Delta\|_2$ and will be positive when
\begin{equation}
    \|\Delta\|_2^2 \ge \delta^2 = 9\frac{\lambda^2}{\kappa^2}\Psi^2(\cMbar)
        + \frac{\lambda}{\kappa}[2\tau^2(\theta^*)+4R(\theta^*_{\cM^\perp})].
\end{equation}
\end{proof}

\begin{rmk}\leavevmode
\begin{itemize}
    \item This is family of bounds, depending on choice of $(\cM, \cMbar^\perp)$.
    \item Error comprises of estimation error $9\frac{\lambda^2}{\kappa^2}\Psi^2(\cMbar)$ and
        approximation error $\frac{4\lambda}{\kappa}R(\theta^*_{\cM^\perp})$.
        $\cM$ ``bigger'' $\Rightarrow$ approx. error $\downarrow$ and est. error $\uparrow$.
        Note that if $\theta^*\in\cM$, then $R(\theta^*_{\cM^\perp})=0$.
    \item To obtain more concrete conclusions, we essentially need to estimate
        all the parameters in the theorem.
\end{itemize}
\end{rmk}

\subsection{Finale}

Application to sparse regression.

\subsubsection{Standard Linear Model}

\begin{itemize}
    \item $Z_i=(x_i,y_i)\in\real^d\times\real$ covariate-response pair.
    \item $y=X\theta^*+w$,
        where $\theta^*\in\real^d$ is an unknown regression vector and
        $X\in\real^{n\times d}$ is the design matrix with $x_i^T$ as $i$-th row.
        $w\in\real^n$ is the noise vector.
    \item model: exactly sparse $\theta^*$;
        \ie, you know the vector is supported on a given set.
        LASSO (least absolute shrinkage and selection operator):
        \begin{equation}
            \thetahat\in\argmin_{\theta\in\real^d}(\frac{1}{2n}\|y-X\theta\|_2^2+\lambda\|\theta\|_1).
        \end{equation}
        Denote
        \begin{equation}
            \cL(\theta;\{z_i\}_{i=1}^n) := \frac{1}{2n}\|y-X\theta\|_2^2.
        \end{equation}
\end{itemize}

\subsubsection{Observe}

\begin{equation}
\begin{split}
      & \cL(\theta^*+\Delta)-\cL(\theta^*)-\nabla\cL(\theta^*)^T\Delta    \\
    = & \frac{1}{2n}(\|y-X(\theta^*+\Delta)\|_2^2-\|y-X\theta^*\|_2^2)
        - \frac{1}{n}(X^TX\theta^*-X^Ty)^T\Delta  \\
    = & \frac{1}{2n}(\|X\Delta\|_2^2+2(\theta^*)^TX^TX\Delta-2y^TX\Delta)
        - \frac{1}{n}(X^TX\theta^*-X^Ty)^T\Delta  \\
    = & \frac{1}{2n}\|X\Delta\|_2^2.
\end{split}
\end{equation}
Thus to establish RSC with
\begin{equation}
    \cM(S) = \cMbar(S) = \{\theta\in\real^d:\theta_j=0, \forall j\notin S\},
\end{equation}
where $S$ is the support of $\theta^*$,
it suffices to prove
\begin{equation}
    \frac{\|X\Delta\|_2^2}{2n}\ge \kappa\|\Delta\|_2^2,\quad
        \forall \Delta\in\cC=\{\Delta\in\real^d:\|\Delta_{S^C}\|_1\le3\|\Delta_S\|_1\}.
\end{equation}
This condition can be viewed as a \emph{restricted eigenvalue condition}.

\subsubsection{Implications}

Before we discuss the validity of the condition,
let us see its implications.

\begin{pro}
    Suppose we have the linear model $y=X\theta^*+w$ and
    \begin{itemize}
        \item $\theta^*$ is $S$-sparse,
        \item The RSC condition holds,
        \item $\frac{\|X_j\|_2}{\sqrt{n}}\le 1, \forall j$,
            where $X_j$ is the $j$-th row of $X$.
        \item $w$ is zero-mean with sub-Gaussian tails; \ie
            $\exists \sigma>0$, \st for any fixed $\|v\|_2=1$,
            \begin{equation}
                \Pr[|v^Tw|\ge t] \le 2\exp(-\frac{t^2}{2\sigma^2}).
            \end{equation}
    \end{itemize}
    Then for $\lambda=4\sigma\sqrt{\frac{\log d}{n}}$, we have
    \begin{equation}
        \|\thetahat-\theta^*\|_2^2\le \frac{144\sigma^2}{\kappa^2}\frac{S\log d}{n},
    \end{equation}
    with prob. $\ge 1-2/d$.
\end{pro}

\begin{proof}
    Since $S$ is chosen as the support of $\theta^*$,
    we have $\theta^*_{\cM(S)^\perp}=0$.
    Also we have $\tau=0$.
    Hence by theorem,
    it remains to bound $\lambda$ and $\Psi(\cMbar(S))=\Psi(\cM(S))$.
    For the latter we have
    \begin{equation}
        \Psi(\cM(S)) = \sup_{u\in\cM(S)\backslash\{0\}}\frac{\|u\|_1}{\|u\|_2}=\sqrt{s}.
    \end{equation}
    For the former, note that
    \begin{equation}
        \nabla\cL(\theta^*) = \frac{1}{n}X^T(X\theta^*-y)=-\frac{1}{n}X^Tw,
    \end{equation}
    and $R^*(v)=\|v\|_\infty$.
    By theorem we need
    \begin{equation}
        \lambda\ge 2R^*(\nabla\cL(\theta^*))=\frac{2}{n}\|X^Tw\|_\infty.
    \end{equation}
    Note that by assumption,
    \begin{equation}
        \Pr\bigg[\bigg|\frac{X_j^T w}{n}\bigg|\ge t\bigg]
            = \Pr\bigg[\bigg|\bigg(\frac{X_j}{\sqrt{n}}\bigg)^T w\bigg|\ge \sqrt{n}t\bigg]
            \le 2\exp\bigg(\!\!-\frac{nt^2}{2\sigma^2}\bigg).
    \end{equation}
    Hence by union bound,
    \begin{equation}
        \Pr\bigg[\bigg\|\frac{X^T w}{n}\bigg\|_\infty\ge t\bigg]
             \le 2d\exp\bigg(\!\!-\frac{nt^2}{2\sigma^2}\bigg).
    \end{equation}
    Let $t^2=(4\sigma^2\log d)/n$. Then
    \begin{equation}
        \Pr\bigg[\bigg\|\frac{X^T w}{n}\bigg\|_\infty\ge t\bigg] \le \frac{2}{d}.
    \end{equation}
    Or equivalently, with prob. $\ge 1-2/d$,
    \begin{equation}
        \lambda\ge \frac{2}{n}\|X^Tw\|_\infty.
    \end{equation}
\end{proof}

\subsubsection{Restricted Eigenvalue Condition}

The remaining question now is whether the restricted eigenvalue condition holds.

\begin{thm}
Suppose that the rows of $X$ are iid $\cN(0,\Sigma)$.
Then $\exists$ constants $c_1,c_2>0$ \st
\begin{equation}
    \frac{\|X\Delta\|_2}{\sqrt{n}}\ge\frac{1}{4}\|\Sigma^{\frac{1}{2}}\Delta\|_2-9\rho(\Sigma)\sqrt{\frac{\log d}{n}}\|\Delta\|_1, \quad \forall\Delta\in\real^d,
\end{equation}
with prob. $\ge 1-c_1\exp(-c_2n)$, where
\begin{equation}
    \rho^2(\Sigma):=\max_{1\le j\le d}\Sigma_{jj}.
\end{equation}
\end{thm}

\begin{rmk} \leavevmode
\begin{itemize}
    \item Note that $\forall \Delta\in\cC$,
        \begin{equation}
            \|\Delta\|_1=\|\Delta_S\|_1+\|\Delta_{S^c}\|_1\le4\|\Delta_S\|_1\le4\sqrt{s}\|\Delta_S\|_2.
        \end{equation}
        Thus if for simplicity, $\Sigma=I$, then
        \begin{equation}
        \begin{split}
            \frac{\|X\Delta\|_2}{\sqrt{n}}
                & \ge\frac{1}{4}\|\Delta\|_2-9\sqrt{\frac{\log d}{n}}(4\sqrt{s}\|\Delta\|_2)    \\
                & =\bigg(\frac{1}{4}-36\sqrt{\frac{s\log d}{n}}\bigg)\|\Delta\|_2, \quad \forall\Delta\in\cC,
        \end{split}
        \end{equation}
        Hence provided that
        \begin{equation}
            \kappa'=\frac{1}{4}-36\sqrt{\frac{s\log d}{n}}>0\quad\Leftrightarrow\quad n>144^2 s\log d,
        \end{equation}
        the restricted eigenvalue condition holds with probability,
        where $\kappa=(\kappa')^2/2$.
    \item The theorem says the nullspace of $X$ cannot have vectors that are too sparse.
    \item Subtlety in the proof of theorem:
        the inequality has to hold for all $\Delta\in\real^d$,
        not just for a fixed $\Delta$.
        This requires tools from random matrix and empirical process theory.
\end{itemize}
\end{rmk}
