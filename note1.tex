\section{Note 1}

\subsection{Prelude}

\subsubsection{Recall}

Regularized loss minimization
% Q
\footnote{examples?}
\begin{equation}
\hat\theta \in \argmin_{\theta\in\real^d} \{\cL(\theta;\{z_i\}_{i=1}^n) + \lambda R(\theta)\}
\end{equation}
for estimating a certain unknown vector $\theta^*\in\real^d$.
% Q
\footnote{def of $\theta^*$?}
Here $z_1,\dots,z_n$ are iid \wrt $\Pbb_{\theta^*}$.
% Q
\footnote{what does notation $\Pbb_{\theta^*}$ mean?}
Denote
\begin{equation}
F := \cL(\theta;\{z_i\}_{i=1}^n) + \lambda R(\theta).
\end{equation}

\subsubsection{This Lecture}

Derive bounds on the statistical error
% Q
\footnote{what is a \emph{statistical error}?}
$\hat\Delta = \hat\theta - \theta^*$.

\subsubsection{Motivation}

\begin{itemize}
    \item From optimization, expect
        $|F(\hat\theta) - F(\theta^*)|$
        to be small.
        % Q
        \footnote{why it should small? why from optimization?}
    \item Show that $\hat\Delta$ belongs to a region
        that $F$ is not too flat.
        % Q
        \footnote{we want $\hat\Delta$ be small,
        so we expect $F$ not flat near origin?}
        This involves accounting fo the effect of $R$.
    \item $R$ is intended to promote certain desirable structure (\eg sparsity).
        We need to know how $R$ penalizes deviation,
        which leads to the \emph{decomposability} concept.
\end{itemize}

\subsubsection{Decomposability of $R$}

\begin{itemize}
    \item $\cM$: model subspace,
        which captures the constraints specified by the model
        (\eg vectors within certain support for sparsity).
    \item Let $\cM \subset \cMbar \subset \real^d$ be a pair of subspaces.
        The \emph{perturbation subspace} is
        \begin{equation}
            \cMbar^\perp = \{v\in\real^d: u^Tv=0, \forall u\in\cMbar\}.
        \end{equation}
\end{itemize}

\begin{define}
    $R$ is decomposable \wrt $(\cM, \cMbar^\perp)$ if
    \begin{equation}
        R(\theta+\gamma) = R(\theta) + R(\gamma) \quad
        \forall \theta \in \cM, \gamma \in \cMbar^\perp.
    \end{equation}
\end{define}

\begin{obs} \leavevmode
\begin{itemize}
    \item $R$ is a norm and hence
        $R(\theta+\gamma)\le R(\theta) + R(\gamma)$.
        This means we want the perturbation term $\gamma$ be
        maximally penalized (up to the equal sign).
        % N
        \footnote{seems to me this maximally penalization is just an excuse for
        the simplified assumption.}
    \item The notion of decomposability is useful when $\cM$ is small,
        in the sense that $\Pi_\cM(\theta^*)\approx\theta^*$.
        % N
        \footnote{i think the sense of small is strange since when
        $\cM$ is large enough, that projection would be exact.
        Should we phrase it as
        \emph{it is useful when $\cM$ is small and
        at the same time not being too far from $\theta^*$}?}
\end{itemize}
\end{obs}

\begin{ex}[Sparse vectors and $l_1$-regularization] \leavevmode
\begin{itemize}
    \item model: $S$-sparse vectors in $\real^d$, $R(\theta)=\|\theta\|_1$.
    \item for any $S\subset\{1,\dots,d\}$ of cardinality $s$,
        define $\cM(S):=\{\theta\in\real^d:\theta_j=0\ \forall j \notin S \}$.
        If $\theta^*$ is supported on $S$,
        then $\Pi_{\cM(S)}(\theta^*)=\theta^*$.
    \item take $\cMbar(S):=\cM(S)$,
        so $\cMbar(S)^\perp = \cM(S)^\perp$.
        % N
        \footnote{notation $\cMbar(S)^\perp$ corrected from $\cMbar^\perp(S)$ (undefined).}
    \item decomposability is obvious.
\end{itemize}
\end{ex}

\begin{ex}[Group-structured norms] \leavevmode
\begin{itemize}
    \item motivation: groups of coefficients likely to be
        zero or non-zero simultaneously.
    \item model: partition $\{1,\dots,d\}$ into $N$ disjoint groups
        $\{G_1,\dots,G_N\}$, number of selected groups should be small,
        say $s$; \ie $S\subset \{1,\dots,N\}$
        are the group indices that correspond to non-zero groups and $s=|S|$.
        The norm is
        \begin{equation}
            R(\theta) = \|\theta\|_{g,p} = \sum_{i=1}^N\|\theta_{G_i}\|_p,\quad p \in[1,\infty].
        \end{equation}
    \item subspace: for any $S\subset\{1,\dots,N\}$, set
        \begin{gather}
            \cM(S) := \{\theta\in\real^d:\theta_{G_i}=0, \forall i\notin S\}, \\
            \cMbar(S) := \cM(S).
        \end{gather}
    \item decomposability can be verified.
\end{itemize}
\end{ex}

\subsubsection{Key Consequences of Decomposability}

\begin{pro} Suppose $\cL$ is convex, differentiable,
    $\lambda\ge 2R^*(\nabla\cL(\theta^*;\{z_i\}_{i=1}^n))$,
    where $R^*(v):=\sup_{R(u)\le 1}u^T v$.
    Then for any $(\cM, \cMbar^\perp)$ over which $R$ is decomposable,
    $\Deltahat = \thetahat - \theta^*$ belongs to
    \begin{equation}
        \cC=\{\Delta\in\real^d:R(\Delta_{\cMbar^\perp})\le 3 R(\Delta_{\cMbar}) + 4R(\theta^*_{\cM^\perp})\},
    \end{equation}
    where $\Delta_{\cMbar^\perp}=\Pi_{\cMbar^\perp}(\Delta)$.
\end{pro}

\begin{proof}
    Define
    \begin{equation}
        \cD(\Delta)=\cL(\theta^*+\Delta)-\cL(\theta^*)+\lambda(R(\theta^*+\Delta)-R(\theta^*)).
    \end{equation}
    Noting optimality of $\thetahat$, we have $\cD(\Deltahat)\le0$.

    \begin{claim}
        \begin{equation}
            R(\theta^*+\Delta)-R(\theta^*) \ge R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp}).
        \end{equation}
    \end{claim}

    \begin{proof}
        Split $\theta^*=\theta_{\cM}^*+\theta^*_{\cM^\perp}$ and $\Delta=\Delta_{\cMbar^\perp}+\Delta_{\cMbar}$,
        \begin{equation}
            \begin{split}
                R(\theta^*+\Delta) - R(\theta^*) & = R(\theta_{\cM}^*+\theta^*_{\cM^\perp}+\Delta_{\cMbar}+\Delta_{\cMbar^\perp}) - R(\theta_{\cM}^*+\theta^*_{\cM^\perp}) \\
                    & \ge (R(\theta_{\cM}^*+\Delta_{\cMbar^\perp})-R(\theta^*_{\cM^\perp}) - R(\Delta_{\cMbar})) - (R(\theta_{\cM}^*) + R(\theta_{\cM^\perp}^*))  \\
                    & = R(\Delta_{\cMbar^\perp})-2R(\theta^*_{\cM^\perp}) - R(\Delta_{\cMbar}).
            \end{split}
        \end{equation}
    \end{proof}

    \begin{claim}
        If $\lambda\ge 2R^*(\nabla\cL(\theta^*))$ and $\cL$ is convex, then
        \begin{equation}
            \cL(\theta^*+\Delta) -\cL(\theta^*) \ge -\frac{\lambda}{2} (R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp})).
        \end{equation}
    \end{claim}

    \begin{proof}
        By convexity and definition of dual norm,
        \begin{align}
            \cL(\theta^*+\Delta)-\cL(\theta^*) & \ge \nabla\cL(\theta^*)^T\Delta \\
                & \ge-|\nabla\cL(\theta^*)^T\Delta| \\
                & \ge -R^*(\nabla\cL(\theta^*))R(\Delta)    \\
                & \ge -\frac{\lambda}{2}(R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp}))
        \end{align}
    \end{proof}

    With these two claims, we have
    \begin{equation}
    \begin{split}
    0 \ge \cD(\Deltahat) & \ge \lambda[R(\Delta_{\cMbar^\perp})-R(\Delta_{\cMbar})-2R(\theta^*_{\cM^\perp})]
        -\frac{\lambda}{2}[R(\Delta_{\cMbar})+R(\Delta_{\cMbar^\perp})]  \\
    & = \frac{\lambda}{2}[R(\Delta_{\cMbar^\perp})-3R(\Delta_{\cMbar})-4R(\theta^*_{\cM^\perp})].
    \end{split}
    \end{equation}
\end{proof}

Now we want $\cL$ to be not too flat in $\cC$.
To formulate this, we use \emph{Restricted Strongly Convex} (RSC).

\begin{define}
    $\cL$ satisfies RSC
    % Q
    \footnote{does RSC tightly related to $\cC$?}
    if $\exists$ constant $\kappa > 0$ and function $\tau(\cdot)$
    % Q
    \footnote{what requirements for $\tau$?}
    \st
    \begin{equation}
        \cL(\theta^*+\Delta)\ge \cL(\theta^*) + \nabla \cL(\theta^*)^T\Delta + \kappa\|\Delta\|_2^2-\tau^2(\theta^*),\quad \forall \Delta\in\cC.
    \end{equation}
\end{define}

Here comes the main theorem.

\begin{thm}
Suppose
\begin{itemize}
    \item $R$ is a decomposable norm \wrt $(\cM, \cMbar^\perp)$,
    \item $\cL$ is convex, differentiable, and RSC,
    \item $\lambda\ge 2R^*(\nabla\cL(\theta^*;\{z_i\}_{i=1}^n))$.
\end{itemize}
Then
\begin{equation}
    \|\thetahat-\theta^*\|_2^2\le 9\frac{\lambda^2}{\kappa^2}\Psi^2(\cMbar)
        + \frac{\lambda}{\kappa}\{2\tau^2(\theta^*)+4R(\theta^*_{\cM^\perp})\},
\end{equation}
where
\begin{equation}
    \Psi(\cM)=\sup_{u\in\cM\backslash\{0\}}\frac{R(u)}{\|u\|_2},
\end{equation}
is the Lipschitz constant of $R$ restricted to $\cM$.
\end{thm}

\begin{proof}
    Let $\delta>0$ be a parameter. Define
    \begin{equation}
        K(\delta) = \cC \cap \{\Delta:\|\Delta\|_2=\delta\}.
    \end{equation}

\begin{claim}
    If $\forall \Delta\in K(\delta), \cD(\Delta)>0$, then $\|\Deltahat\|_2\le\delta$.
\end{claim}

\begin{proof}

\begin{claim}
    $\cC$ is star-shaped; \ie $\forall \Delta\in\cC, \{t\Delta:t\in(0,1)\}\subset \cC$.
\end{claim}

\begin{proof}
If $\theta^*\in\cM$, $\cC$ is a cone.
Now suppose $\theta^*\notin\cM$.
Since $\cMbar$ is a subspace and projection $\Pi_{\cMbar}$ is linear,
we have
\begin{equation}
    \Pi_{\cMbar}(t\Delta) = t\Pi_{\cMbar}(\Delta), \quad
    \Pi_{\cMbar^\perp}(t\Delta) = t\Pi_{\cMbar^\perp}(\Delta).
\end{equation}
Hence for $t\in(0,1)$,
\begin{equation}
\begin{split}
    R(\Pi_{\cMbar^\perp}(t\Delta)) & = tR(\Pi_{\cMbar^\perp}(\Delta)) \\
        & \le t(3 R(\Pi_{\cMbar}(\Delta)) + 4R(\theta^*_{\cM^\perp}))    \\
        & \le 3 R(\Pi_{\cMbar}(t\Delta)) + 4R(\theta^*_{\cM^\perp}).
\end{split}
\end{equation}
\end{proof}

Suppose $\|\Deltahat\|_2>\delta$.
Then the line joining $0$ and $\Deltahat$ will intersect $K(\delta)$
at some $t^*\Deltahat$, where $t^*\in(0,1)$.
Since $\cD$ is convex and $\cD(0)=0$,
$\cD(t^*\Deltahat) \le t^*D(\Deltahat) \le 0$,
which is a contradiction.
\end{proof}


\end{proof}

\begin{rmk}\leavevmode
\begin{itemize}
    \item This is family of bounds, depending on choice of $(\cM, \cMbar^\perp)$.
    \item Error comprises of estimation error $9\frac{\lambda^2}{\kappa^2}\Psi^2(\cMbar)$ and
        approximation error $\frac{4\lambda}{\kappa}R(\theta^*_{\cM^\perp})$.
        $\cM$ ``bigger'' $\Rightarrow$ approx. error $\downarrow$ and est. error $\uparrow$.
        Note that if $\theta^*\in\cM$, then $R(\theta^*_{\cM^\perp})=0$.
    \item To obtain more concrete conclusions, we essentially need to estimate
        all the parameters in the theorem.
\end{itemize}
\end{rmk}
