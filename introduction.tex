\section{Introduction} \label{sec:intro}

Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative}
as generative models have been actively studied
and developed~\cite{chen2016infogan,nowozin2016f,
arjovsky2017wasserstein,zhao2016energy,radford2015unsupervised,
mescheder2017adversarial,mirza2014conditional,
odena2016conditional,denton2015deep,
huang2016stacked,zhang2016stackgan,kim2017learning,zhu2017unpaired,
che2016mode,donahue2016adversarial,salimans2016improved,zhu2016generative}
in the last few years.
There are theoretical discussions~\cite{arjovsky2017wasserstein,
zhao2016energy,nowozin2016f},
various extensions~\cite{chen2016infogan,che2016mode,donahue2016adversarial,
salimans2016improved,mescheder2017adversarial,mirza2014conditional,
huang2016stacked},
exploring effective network design and
training methods~\cite{radford2015unsupervised,arjovsky2017towards},
and applications in image generation~\cite{odena2016conditional,denton2015deep,
zhang2016stackgan},
manipulation~\cite{zhu2016generative,perarnau2016invertible},
and cross domain transfer~\cite{kim2017learning,zhu2017unpaired},
and many others.
Compared to other generative models,
like Restricted Boltzmann Machine~\cite{hinton2002training},
Variational Auto-Encoders~\cite{kingma2013auto}, etc.,
GAN is reported to be able to generate higher quality examples.
For example,
in applications in super-resolution~\cite{ledig2016photo},
the generated images are visually more sharp with adversarial losses.
This is one of the advantages of the GAN formulation.
However, in general,
GAN also faces some well known problems:
hard to train,
mode collapse,
lack of systematic evaluations methods,
to name a few.

GAN is formulated as a two-player game that involves a generator and
a discriminator.
Given a data distribution that we want to model,
the generator is trained to generate samples that look real,
while the discriminator is trained to distinguish between
samples that come from real data distribution and
those come from the generator.
This is a dynamic process.
In each training cycle,
both the generator and the discriminator have to adjust themselves
to cope with the changes in their opponents.
In the equilibrium state,
the discriminator cannot identify the source of a sample,
and the generator is able to generate samples that share the same
distribution as the real data distribution.
Formally,
GAN is formulated~\cite{goodfellow2014generative} as the following problem,
\begin{equation}\label{eq:gan}
    \min_G \max_D \Ebb_{x\sim p_{\text{data}}(x)}[\log D(x)]
        + \Ebb_{z\sim p_z(z)}[\log (1-D(G(z)))],
\end{equation}
where $G$ is the generator neural network,
$D$ is the discriminator neural network,
$x$ is a sample from data distribution,
and $z$ is a sample from noise distribution.
If the discriminator is certain that a sample $x$ comes from
data distribution,
then the output should be $1$.
If it is certain that a fake sample comes,
the output of $D$ is $0$.
In the equilibrium state,
when the discriminator $D$ cannot decide where the sample comes from,
the output is $1/2$.
It can be shown that if in each step,
the discriminator is at its optimum,
then the objective of minimizing generator $G$ in Equation~\eqref{eq:gan}
is equivalent to minimizing the Jensen-Shannon divergence between
the real data distribution and the generated distribution.
Note that the theoretical justification of GAN
only considers an idealized setting,
in which the discriminator is able to achieve the optimum,
and the capacities of networks are unlimited.
These assumptions never hold in real experiments.
So in contrast to the utopian world that the theory demonstrates,
neither are we guaranteed that the perfect distribution
can be generated in the end,
nor can we be confident to say that the algorithm would converge.

Although the original formulation of GAN
adopts a game theory and probabilistic point of view,
it is also natural to interpret the training dynamics of GAN
in the context of manifold learning.
In most real applications,
we are dealing with structured data,
such as natural images.
The distribution of such data may lie on a low dimensional manifold
instead of distributing all over the space.
So we can view the target of GAN is to learn such a data manifold
which is parametrized by the latent space.
The authors of Wasserstein GAN~\cite{arjovsky2017towards,arjovsky2017wasserstein}
use this point of view to explain
why GANs are so hard to train in practice.
The core idea in their argument is that,
since the dimension of the latent space and the dimension of the data manifold
are usually smaller than the data space,
we are trying to matching two low dimensional manifolds in a high dimensional
ambient space.
The chance of two such manifolds have sufficient overlap is zero.
In this case,
the Jensen Shannon divergence do not even have a valid definition.
This explains why training GAN is so difficult.

Since this close relationship between geometry and GAN,
we are inspired to investigate more deep into the geometric aspects of GAN.
There are many important concepts for a manifold,
such as dimensionality,
connectivity,
and the topology.
All those aspects lack sufficient discussion in current literature.
In this report,
we study the effect of these geometric properties on the training of GAN.

\subsection{Related Work}

The geometric viewpoint appears in many works on
GAN~\cite{arjovsky2017towards,zhao2016energy,
arjovsky2017wasserstein,che2016mode,zhu2017unpaired,zhu2016generative}.
As discussed above,
the geometric view can be used to explain the difficulties encountered in
training GAN~\cite{arjovsky2017towards}.
WGAN~\cite{arjovsky2017wasserstein} exploits the good behavior of
Wasserstein distance in measuring two distributions concentrated on
low-dimensional manifolds.
In~\cite{white2016sampling},
the author try to convince us that a spherical latent space is
better than a cube.
And we should notice the topological difference of
these two latent space structures.
Mode regularized GAN~\cite{che2016mode} introduces mode regularizer and
manifold-diffusion training based on the geometric intuition.
The geometric viewpoint is also implicitly referred in many other works.
Except for a few papers that pursue a mathematically rigorous style,
most of them use terminologies such as manifold,
in a casual way.
Nevertheless,
the geometric viewpoint gives a good intuition for us to understand
what GAN is doing and facilitates inspirations for new structures and algorithms.
