\section{Introduction} \label{sec:intro}

Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative}
as generative models have been actively studied
and developed~\cite{chen2016infogan,nowozin2016f,
arjovsky2017wasserstein,zhao2016energy,radford2015unsupervised,
mescheder2017adversarial,mirza2014conditional,gauthier2014conditional,
odena2016conditional,denton2015deep,reed2016generative,
huang2016stacked,zhang2016stackgan,kim2017learning,zhu2017unpaired,
che2016mode,donahue2016adversarial,salimans2016improved,zhu2016generative}
in the last few years.
There are theoretical discussions~\cite{arjovsky2017wasserstein,
zhao2016energy,nowozin2016f},
various extensions~\cite{chen2016infogan,che2016mode,donahue2016adversarial,
salimans2016improved,mescheder2017adversarial,mirza2014conditional,
gauthier2014conditional,huang2016stacked},
exploring effective network design and
training methods~\cite{radford2015unsupervised},
and applications in image generation~\cite{odena2016conditional,denton2015deep,
reed2016generative,zhang2016stackgan},
manipulation~\cite{zhu2016generative},
and cross domain transfer~\cite{kim2017learning,zhu2017unpaired},
and many others.
Compared to other generative models,
like Restricted Boltzmann Machine~\cite{hinton2002training},
Variational Auto-Encoders~\cite{kingma2013auto}, etc.,
GAN is reported to be able to generate higher quality examples.
For example,
in applications in super-resolution~\cite{ledig2016photo},
the generated images are visually more sharp with adversarial losses.
This is one of the advantages of the GAN formulation.
However, in general,
GAN also faces some well known problems:
hard to train,
mode collapse,
lack of systematic evaluations methods,
to name a few.

\subsection{Related Work}
