\section{Introduction} \label{sec:intro}

Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative}
as generative models have been actively studied
and developed~\cite{chen2016infogan,nowozin2016f,
arjovsky2017wasserstein,zhao2016energy,radford2015unsupervised,
mescheder2017adversarial,mirza2014conditional,
odena2016conditional,denton2015deep,
huang2016stacked,zhang2016stackgan,kim2017learning,zhu2017unpaired,
che2016mode,donahue2016adversarial,salimans2016improved,zhu2016generative}
in the last few years.
There are theoretical discussions~\cite{arjovsky2017wasserstein,
zhao2016energy,nowozin2016f},
various extensions~\cite{chen2016infogan,che2016mode,donahue2016adversarial,
salimans2016improved,mescheder2017adversarial,mirza2014conditional,
huang2016stacked},
exploring effective network design and
training methods~\cite{radford2015unsupervised,arjovsky2017towards},
and applications in image generation~\cite{odena2016conditional,denton2015deep,
zhang2016stackgan},
manipulation~\cite{zhu2016generative,perarnau2016invertible},
and cross domain transfer~\cite{kim2017learning,zhu2017unpaired},
etc.
Compared to Restricted Boltzmann Machines~\cite{hinton2002training},
Variational Auto-Encoders~\cite{kingma2013auto},
or other generative models,
GAN is able to generate higher quality examples~\cite{ledig2016photo}
However,
GAN also faces some well known problems,
such as hard to train,
mode collapse,
and lack of systematic evaluation methods,
to name a few.

GAN is formulated as a two-player game that involves a generator $G$ and
a discriminator $D$.
Given a data distribution that we want to model,
the generator is trained to generate samples that look real,
while the discriminator is trained to distinguish between
samples that come from real data distribution and
those come from the generator.
It can be shown~\cite{goodfellow2014generative} that if in each step,
the discriminator is at its optimum,
then the objective of GAN
is equivalent to minimizing the Jensen-Shannon divergence between
the real data distribution and the generated sample distribution.
At the equilibrium state,
the discriminator cannot identify the source of a sample,
and the generator is able to generate samples that share the same
distribution as the real data.

It is also natural to interpret GAN
in the context of manifold learning.
In real applications,
we are dealing with structured data
such as natural images.
The distribution of such data often concentrated on a low dimensional manifold.
So we can view the target of GAN is to learn such a data manifold
which is parametrized by the latent space.
The authors of Wasserstein GAN~\cite{arjovsky2017towards,arjovsky2017wasserstein}
adopt this interpretation to explain why GANs are so hard to train.
Intuitively speaking,
both the data manifold and the generated manifold are
low dimensional manifolds in a high dimensional ambient space,
which means that they almost never have sufficient overlap.
In this case,
the Jensen Shannon divergence will have trouble by definition
and this accounts for the difficulties in training.
This evidenced that the geometric view is helpful in understanding GAN.

Since this close relationship between geometry and GAN,
we are inspired to investigate more deep into the geometric aspects of GAN.
There are many important concepts for a manifold,
for example the dimensionality and the connectivity.
All these aspects lack sufficient discussion in current literature.
In this report,
we study the effect of these geometric properties on the training of GAN.

\subsection{Related Work}

The geometric viewpoint appears in many works on
GAN~\cite{arjovsky2017towards,zhao2016energy,
arjovsky2017wasserstein,che2016mode,zhu2017unpaired,zhu2016generative}.
As discussed above,
it explains the difficulties encountered in
training GAN~\cite{arjovsky2017towards}.
WGAN~\cite{arjovsky2017wasserstein} exploits the good behavior of the
Wasserstein distance in measuring two distributions that are concentrated on
low-dimensional manifolds.
In~\cite{white2016sampling},
the author try to convince us that a spherical latent space is
better than a cube.
Notice  the topological difference between
cube and sphere in this case.
Mode regularized GAN~\cite{che2016mode} introduces mode regularizer and
manifold-diffusion training based on the geometric interpretation.
The geometric viewpoint is also implicitly referred to in many works
through the casual usage of terminologies like manifold.
The benefit is to provide a clear intuition that facilitate understandings.
