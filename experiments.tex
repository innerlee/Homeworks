\section{Experiments and Visualization} \label{sec:exp}

To answer the questions raised in previous section,
we specially designed some learning problems in low-dimensions,
so that we can visualize what happens in GAN.
Both the latent space and the data space is
restricted to $1$-dimension or $2$-dimension.
We use \emph{fully connected} (fc) layers as the building blocks for both
the generator and the discriminator.
A \emph{batch normalization} layer and a \emph{Elu} activation layer
follow after each fc layer.
Across all experiments,
Both the generator and discriminator have
$20-40-100-200-200-100-40-20$ hidden neuron numbers.
The capacity of these networks is large enough for our experiments.
We use \emph{rmsprop} with initial lr $0.001$,
weight decay $0.0005$, and batch size $256$
as the optimization strategy.
The learning rate is multiplied by $0.98$ after every $100$ iterations.
Generator and discriminator are updated alternately,
with each processing $1$ batch.

We use \emph{Parrots}~\cite{parrots} as the deep learning framework
and use its Julia port \emph{Parrots.jl}~\cite{parrotsjl}
as the working language.
All codes were implemented from scratch based on
the algorithm described in~\cite{goodfellow2014generative}.
Codes and detailed results are available at
\url{https://github.com/innerlee/ELEG5491}.

\subsection{Dimensionality}

\begin{figure}[ht]
\centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/spiral_gaussgauss_00030000"}
        \caption{$z$: $1$-dim, spiral.
            $x$: $2$-dim, Gaussian.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/lattice3x3_gaussgauss_00030000"}
        \caption{$z$: $0$-dim, lattice.
            $x$: $2$-dim, Gaussian.}
    \end{subfigure}
    \vskip 0.4cm
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/gaussgauss_spiral_00030000"}
        \caption{$z$: $2$-dim, Gaussian.
            $x$: $1$-dim, spiral.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/gaussgauss_lattice3x3_00030000"}
        \caption{$z$: $2$-dim, Gaussian.
            $x$: $0$-dim, lattice.}
    \end{subfigure}
    \caption{\small
        GAN in different dimensions.
        The snapshots are taken at iteration $30000$.
        }
    \label{fig:dim}
\end{figure}

We use three manifolds:
1) A $3\times3$ lattice. It is $0$-dimensional manifold in $\real^2$.
2) A $1$-dimensional spiral curve in $\real^2$.
3) the whole $2$-dimensional plane with Gaussian distribution.
As shown in Figure~\ref{fig:dim},
we can see that
1) If the dimension of latent manifold is lower than that of the data manifold
(Figure~\ref{fig:dim}~(a, b)),
the generated low dimensional manifold try to cover the
larger dimensional data manifold as possible.
2) If the dimension of latent manifold is larger (Figure~\ref{fig:dim}~(c, d)),
then the generated manifold is able to cover the data manifold.
However,
it may generate many fake examples that do not belong to the data manifold.

\subsection{Connectivity}

\begin{figure}[ht]
\centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/uniformuniform_patch3x3_00030000"}
        \caption{$z$: $2$-dim, uniform square, connected.\\
            $x$: $2$-dim, 9 square patches, disconnected.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/patch3x3_uniformuniform_00030000"}
        \caption{$z$: $2$-dim, 9 square patches, disconnected.\\
            $x$: $2$-dim, uniform square, connected.}
    \end{subfigure}
    \vskip 0.4cm
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/sphere1_fourleaves_00030000"}
        \caption{$z$: $1$-dim, unit circle, connected.\\
            $x$: $1$-dim, four arcs, disconnected.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/fourleaves_sphere1_00030000"}
        \caption{$z$: $1$-dim, four arcs, disconnected.\\
            $x$: $1$-dim, unit circle, connected.}
    \end{subfigure}
    \caption{\small
        GAN in connected/disconnected manifolds.
        The snapshots are taken at iteration $30000$.
        }
    \label{fig:conn}
\end{figure}

We have two settings:
1) A two dimensional square with uniform distribution.
It is a connected manifold.
2) A disconnected manifold that consists 9 small squares as components.
From Figure~\ref{fig:conn},
we can observe that,
1) The connected latent space is able to approximate the disconnected
data manifold (Figure~\ref{fig:conn} (a)).
2) The disconnected latent space has difficulty in covering
the data manifold although the dimensions are the same
(Figure~\ref{fig:conn} (b)).
This may be caused by the continuity in the learned mapping
of the generator.
