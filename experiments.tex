\section{Experiments and Visualization} \label{sec:exp}

To answer the questions raised in previous section,
we specially designed toy problems in low-dimensions,
so that we can visualize what happens in the training dynamics of GAN.
We use \emph{fully connected} layers as the building blocks for both
the generator and discriminator.
After each fully connected layer,
we add a \emph{batch normalization} layer and a \emph{Elu} activation layer.
Both the latent space and the data space is
restricted to $1$-dimension or $2$-dimension.
Across all experiments,
we use fc networks with
$20-40-100-200-200-100-40-20$ hidden neuron numbers
as the structure for generator.
The same hidden neuron setting is also applied to the discriminator.
The capacity of these networks is large enough for our experiments.
We use \emph{rmsprop} with initial lr $0.001$ as the optimization method.
learning rate is multiplied by $0.98$ after every $300$ iterations.
The batch size is set to $256$,
and we update generator and discriminator alternately,
with each processes $1$ batch of data.

We use \emph{Parrots}~\cite{parrots} as the deep learning framework
and use the the Julia port \emph{Parrots.jl}~\cite{parrotsjl}
as the working language.
All codes were implemented from scratch based on
the algorithm described in~\cite{goodfellow2014generative}.
Codes and experiment results are available at
\url{https://github.com/innerlee/ELEG5491}.

\subsection{Dimensionality}

\begin{figure}[ht]
\centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/spiral_gaussgauss_000100"}
        \caption{$z$: $1$-dim, spiral.
            $x$: $2$-dim, Gaussian.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/lattice3x3_gaussgauss_000100"}
        \caption{$z$: $0$-dim, lattice.
            $x$: $2$-dim, Gaussian.}
    \end{subfigure}
    \vskip 0.4cm
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/gaussgauss_spiral_000100"}
        \caption{$z$: $2$-dim, Gaussian.
            $x$: $1$-dim, spiral.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/gaussgauss_lattice3x3_000100"}
        \caption{$z$: $2$-dim, Gaussian.
            $x$: $0$-dim, lattice.}
    \end{subfigure}
    \caption{\small
        GAN in different dimensions.
        The snapshots are taken at iteration $10000$.
        }
    \label{fig:dim}
\end{figure}

We use three manifolds:
1) A $3\times3$ lattice. It is $0$-dimensional manifold in $\real^2$.
2) A $1$-dimensional spiral curve in $\real^2$.
3) the whole $2$-dimensional plane with Gaussian distribution.
As shown in Figure~\ref{fig:dim},
we can see that
1) If the dimension of latent manifold is lower than that of the data manifold
(Figure~\ref{fig:dim}~(a, b)),
the generated low dimensional manifold try to cover the
larger dimensional data manifold as possible.
2) If the dimension of latent manifold is larger (Figure~\ref{fig:dim}~(c, d)),
then the generated manifold is able to cover the data manifold.
However,
it may generate many fake examples that do not belong to the data manifold.

\subsection{Connectivity}

\begin{figure}[ht]
\centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/uniformuniform_patch3x3_000100"}
        \caption{$z$: $2$-dim, uniform square, connected.\\
            $x$: $2$-dim, 9 square patches, disconnected.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{"fig/patch3x3_uniformuniform_000100"}
        \caption{$z$: $2$-dim, 9 square patches, disconnected.\\
            $x$: $2$-dim, uniform square, connected.}
    \end{subfigure}
    \caption{\small
        GAN in connected/disconnected manifolds.
        The snapshots are taken at iteration $10000$.
        }
    \label{fig:conn}
\end{figure}

We have two settings:
1) A two dimensional square with uniform distribution.
It is a connected manifold.
2) A disconnected manifold that consists 9 small squares as components.
From Figure~\ref{fig:conn},
we can observe that,
1) The connected latent space is able to approximate the disconnected
data manifold (Figure~\ref{fig:conn} (a)).
2) The disconnected latent space has difficulty in covering
the data manifold although the dimensions are the same
(Figure~\ref{fig:conn} (b)).
This may be caused by the continuity in the learned mapping
of the generator.

\subsection{Topology}
