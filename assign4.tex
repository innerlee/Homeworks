\clearpage
\section{Assignment 4}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.1

Let $u = x+y, v=x-y$, then

\begin{align}
    \int_0^1\int_0^{1-x}\sqrt{x+y}(y-2x)^2dydx
        &= \int_0^1\int_{-u}^{u}\sqrt{u}\bigg(\frac{u-v}{2}-u-v\bigg)^2 \frac{1}{2} dvdu \\
        &= \frac{1}{8} \int_0^1\int_{-u}^{u} \sqrt{u} (u+3v)^2 dvdu \\
        &= \int_0^1 \sqrt{u} u^3 du \\
        &= \frac{2}{9}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.2

Since $Y_i$ are iid, we have $V=\sum_{i=1}^{k-1} Y_i\sim Gamma\big(\sum_{i=1}^{k-1} \alpha_i, \theta\big)$.
Let $Z_i:=Y_i/V$

\begin{align}
    f(z_1,\dots,z_k) d\sigma
        &\propto \int_0^\infty \prod_{i=1}^{k}\frac{(lz_i)^{\alpha_i-1}e^{-\frac{(lz_i)}{\theta}}}{\theta^{\alpha_i}\Gamma(\alpha_i)} l^{k-1} d\sigma dl \\
        &\propto \prod_{i=1}^{k}\frac{z_i^{\alpha_i-1}}{\theta^{\alpha_i}\Gamma(\alpha_i)} d\sigma  \int_0^\infty l^{(\sum_{i=1}^k\alpha_i) - 1} e^{-\frac{l}{\theta}(\sum_{i=1}^k z_i)}dl \\
        &\propto \Gamma\bigg(\sum_{i=1}^k\alpha_i\bigg)\prod_{i=1}^{k}\frac{z_i^{\alpha_i-1}}{\Gamma(\alpha_i)} d\sigma.
\end{align}

Compare the equation with the form of Dirichlet distribution's pdf,
we know they must be the same.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.3

Let $X_i\sim Pois(1), i=1,\dots,n$ be iid, then $Y_n:=\sum_{i=1}^{n}X_i\sim Pois(n)$.
By CLT,

\begin{align}
    Z_n:=\frac{Y_n-n}{\sqrt{n}} \xrightarrow{d} N(0, 1).
\end{align}

Then,

\begin{equation}
    \sum_{i=0}^n \frac{n^i}{i!}e^{-n} =: P(Y_n\le n) = P(Z_n\le 0) \rightarrow \Phi(0) = \frac{1}{2}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.4

\begin{align}
    Var(Z_iY_i) = E((Z_iY_i)^2) - (E(Z_iY_i))^2 = \sigma^4.
\end{align}
By CLT,
\begin{align}
    \frac{Z_1Y_1+\dots+Z_nY_n}{\sqrt{n}}\xrightarrow{d} N(0, \sigma^4).
\end{align}
By Weak Law of Large Number,
\begin{align}
    \frac{Z_1^2+\dots+Z_n^2}{n}\xrightarrow{p} \sigma^2.
\end{align}
Since function $1/x$ is continuous at $\sigma^2$, we have
\begin{align}
    \frac{n}{Z_1^2+\dots+Z_n^2}\xrightarrow{p} \frac{1}{\sigma^2}.
\end{align}
By Slutsky’s theorem,
\begin{align}
    \sqrt{n}\frac{Z_1Y_1+\dots+Z_nY_n}{Z_1^2+\dots+Z_n^2}\xrightarrow{d}  N(0, 1).
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.5

\begin{align}
    P(X_{(j)}\le x)
        &= \sum_{r=j}^n P(X_{(r)}\le x, X_{(r+1)}>x) \\
        &= \sum_{r=j}^n \binom{n}{r}(F(x))^r(1-F(x))^{n-r}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.6

\subsubsection{} %4.6.1

By CLT,
\begin{align}
    \sqrt{n}(\bar{X_n} - p)\xrightarrow{d} N(0,p(1-p)).
\end{align}

Let $g(x)=\sin^{-1}(\sqrt{x})$,
then $g'(p)=\frac{1}{2\sqrt{p(1-p)}}\ne 0$.
Then by Delta theorem
\begin{align}
    \sqrt{n}(\sin^{-1}\sqrt{\bar{X_n}} - \sin^{-1}\sqrt{p})\xrightarrow{d} N(0,1/4).
\end{align}

\subsubsection{} %4.6.2

\begin{align}
    S_n
        &= \frac{1}{n}\sum_{i=1}^n ((X_i-\mu) - (\bar X - \mu))^2 \\
        &= \frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - (\bar X - \mu)^2.
\end{align}

By Weak Law of Large Number, $\bar X - \mu\xrightarrow{p} 0$.
We have $\frac{(\bar X - \mu)^2}{\sqrt{n}}\xrightarrow{p} 0$ (easy to prove).

Since $\big(\frac{X_i-\mu}{\sigma}\big)^2\sim\chi^2$, for all $i$,
by CLT, we have
\begin{equation}
    \sqrt{n}\bigg(\frac{\sum_{i=1}^n \big(\frac{X_i-\mu}{\sigma}\big)^2}{n} - 1\bigg) \xrightarrow{d} N(0,2).
\end{equation}
So,
\begin{equation}
    \sqrt{n}\bigg(\frac{\sum_{i=1}^n (X_i-\mu)^2}{n} - \sigma^2\bigg) \xrightarrow{d} N(0,2\sigma^4).
\end{equation}

By Slutsky’s theorem,
\begin{equation}
    \sqrt{n}\big(S_n-\sigma^2\big) = \sqrt{n}\bigg(\frac{\sum_{i=1}^n (X_i-\mu)^2}{n} - \sigma^2\bigg) - \frac{(\bar X - \mu)^2}{\sqrt{n}}\xrightarrow{d} N(0,2\sigma^4).
\end{equation}

Let $g(x)=\log(x)$,
then $g'(\sigma^2)=\frac{1}{\sigma^2}\ne 0$.
Then by Delta theorem
\begin{equation}
    \sqrt{n}\big(\log(S_n)-\log(\sigma^2)\big) \xrightarrow{d} N(0,2).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.7

\begin{align}
    E(g'(x))
        &=\int_{-\infty}^{\infty} h(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)g'(x)dx \\
        &=-\int_{-\infty}^{\infty} \bigg(h(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)\bigg)'g(x)dx \\
        &=-\int_{-\infty}^{\infty} g(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)\bigg(h'(x)+h(x)\sum_{i=1}^k w_i(\vtheta)T_i'(x)\bigg)dx \\
        &=-E\bigg[g(x)\bigg(\frac{h'(x)}{h(x)}+\sum_{i=1}^k w_i(\vtheta)T_i'(x)\bigg)\bigg]
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.8

Let $T(\vx):=\sum_{i=1}^n X_i$, then
\begin{equation}
    \frac{P(\vx|\theta)}{P(T(\vx)|\theta)}
        = \frac{\prod_{i=1}^n \theta(1-\theta)^{x_i-1}}{\binom{T(\vx)-1}{n-1}\theta^n(1-\theta)^{T(\vx)-n}}
        = \frac{1}{\binom{T(\vx)-1}{n-1}}.
\end{equation}
So $\sum_{i=1}^n X_i$ is a sufficient statistic.

Let $g$ be a function. Then for $0<\theta<1$, let
\begin{align}
    0 = E_\theta(g(T))
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}\theta^n(1-\theta)^{t-n} \\
        &= \theta^n(1-\theta)^{-n}\sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}(1-\theta)^{t}.
\end{align}
Since $\theta^n(1-\theta)^{-n}\ne0$ when $0<\theta<1$, we have
\begin{align}
    0
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}(1-\theta)^{t} \\
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}r^{t}.
\end{align}
for all $r$, where $0\le r<1$.
Then, $g(t)$ must be $0$ for all $t\ge n$.
If not, let $k$ be the smallest number such that $g(k)\ne 0$.
then we can rewrite as $0=r^k h(r)$, where $h(0)=\binom{k-1}{n-1}g(k)\ne 0$.
There exists an $\epsilon>0$, such that $h(r)\ne 0$ if $0\le r<\epsilon$.
This is a contradiction because $r^k h(r)=0$ for all $0\le r<1$.

We have $P_\theta(g(T)=0)=1$ for all $\theta$.
So, $\sum_{i=1}^n X_i$ is complete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.9

Let $n=\sum_{i=1}^4 x_i$. Denote $\vx=(\vx_1,\dots,\vx_N)$,
and $\vx_t=(x_{1,t}, \dots, x_{4,t})$.
\begin{align}
    f(\vx|\theta)
        &= \prod_{t=1}^N \frac{n!}{x_{1,t}!x_{2,t}!x_{3,t}!x_{4,t}!}
            \bigg(\frac{1}{2}+\frac{\theta}{4}\bigg)^{x_{1,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{x_{2,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{x_{3,t}}
            \bigg(\frac{\theta}{4}\bigg)^{n-\sum_{i=1}^3 x_{i,t}} \\
        &=  h(\vx)
            \bigg(\frac{1}{2}+\frac{\theta}{4}\bigg)^{\sum_{t=1}^N x_{1,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{\sum_{t=1}^N x_{2,t} + x_{3,t}}
            \bigg(\frac{\theta}{4}\bigg)^{nN-\sum_{i=1}^3\sum_{t=1}^N  x_{i,t}}.
\end{align}
By the factorization theorem,
$\big(\sum_{t=1}^N x_{1,t}, \sum_{t=1}^N x_{2,t} + x_{3,t}\big)$
is an sufficient statistic.

To show it is also minimal, we compute
\begin{align}
    \frac{f(\vx|\theta)}{f(\vy|\theta)}
        &= \frac{
            h(\vx)
            \big(\frac{1}{2}+\frac{\theta}{4}\big)^{\sum_{t=1}^N x_{1,t}}
            \big(\frac{1}{4}-\frac{\theta}{4}\big)^{\sum_{t=1}^N x_{2,t} + x_{3,t}}
            \big(\frac{\theta}{4}\big)^{nN-\sum_{i=1}^3\sum_{t=1}^N  x_{i,t}}
        }{
            h(\vy)
            \big(\frac{1}{2}+\frac{\theta}{4}\big)^{\sum_{t=1}^N y_{1,t}}
            \big(\frac{1}{4}-\frac{\theta}{4}\big)^{\sum_{t=1}^N y_{2,t} + y_{3,t}}
            \big(\frac{\theta}{4}\big)^{nN-\sum_{i=1}^3\sum_{t=1}^N  y_{i,t}}
        }.
\end{align}
It is independent of $\theta$ iff
$\sum_{t=1}^N x_{1,t} = \sum_{t=1}^N y_{1,t}$ and
$\sum_{t=1}^N x_{2,t} + x_{3,t} = \sum_{t=1}^N y_{2,t} + y_{3,t}$.
So, $\big(\sum_{t=1}^N x_{1,t}, \sum_{t=1}^N x_{2,t} + x_{3,t}\big)$
is also minimal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.10
