\clearpage
\section{Assignment 4}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.1

Let $u = x+y, v=x-y$, then

\begin{align}
    \int_0^1\int_0^{1-x}\sqrt{x+y}(y-2x)^2dydx
        &= \int_0^1\int_{-u}^{u}\sqrt{u}\bigg(\frac{u-v}{2}-u-v\bigg)^2 \frac{1}{2} dvdu \\
        &= \frac{1}{8} \int_0^1\int_{-u}^{u} \sqrt{u} (u+3v)^2 dvdu \\
        &= \int_0^1 \sqrt{u} u^3 du \\
        &= \frac{2}{9}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.2

Since $Y_i$ are iid, we have $V=\sum_{i=1}^{k-1} Y_i\sim Gamma\big(\sum_{i=1}^{k-1} \alpha_i, \theta\big)$.
Let $Z_i:=Y_i/V$

\begin{align}
    f(z_1,\dots,z_k) d\sigma
        &\propto \int_0^\infty \prod_{i=1}^{k}\frac{(lz_i)^{\alpha_i-1}e^{-\frac{(lz_i)}{\theta}}}{\theta^{\alpha_i}\Gamma(\alpha_i)} l^{k-1} d\sigma dl \\
        &\propto \prod_{i=1}^{k}\frac{z_i^{\alpha_i-1}}{\theta^{\alpha_i}\Gamma(\alpha_i)} d\sigma  \int_0^\infty l^{(\sum_{i=1}^k\alpha_i) - 1} e^{-\frac{l}{\theta}(\sum_{i=1}^k z_i)}dl \\
        &\propto \Gamma\bigg(\sum_{i=1}^k\alpha_i\bigg)\prod_{i=1}^{k}\frac{z_i^{\alpha_i-1}}{\Gamma(\alpha_i)} d\sigma.
\end{align}

Compare the equation with the form of Dirichlet distribution's pdf,
we know they must be the same.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.3

Let $X_i\sim Pois(1), i=1,\dots,n$ be iid, then $Y_n:=\sum_{i=1}^{n}X_i\sim Pois(n)$.
By CLT,

\begin{align}
    Z_n:=\frac{Y_n-n}{\sqrt{n}} \xrightarrow{d} N(0, 1).
\end{align}

Then,

\begin{equation}
    \sum_{i=0}^n \frac{n^i}{i!}e^{-n} =: P(Y_n\le n) = P(Z_n\le 0) \rightarrow \Phi(0) = \frac{1}{2}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.4

\begin{align}
    Var(Z_iY_i) = E((Z_iY_i)^2) - (E(Z_iY_i))^2 = \sigma^4.
\end{align}
By CLT,
\begin{align}
    \frac{Z_1Y_1+\dots+Z_nY_n}{\sqrt{n}}\xrightarrow{d} N(0, \sigma^4).
\end{align}
By Weak Law of Large Number,
\begin{align}
    \frac{Z_1^2+\dots+Z_n^2}{n}\xrightarrow{p} \sigma^2.
\end{align}
Since function $1/x$ is continuous at $\sigma^2$, we have
\begin{align}
    \frac{n}{Z_1^2+\dots+Z_n^2}\xrightarrow{p} \frac{1}{\sigma^2}.
\end{align}
By Slutsky’s theorem,
\begin{align}
    \sqrt{n}\frac{Z_1Y_1+\dots+Z_nY_n}{Z_1^2+\dots+Z_n^2}\xrightarrow{d}  N(0, 1).
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.5

\begin{align}
    P(X_{(j)}\le x)
        &= \sum_{r=j}^n P(X_{(r)}\le x, X_{(r+1)}>x) \\
        &= \sum_{r=j}^n \binom{n}{r}(F(x))^r(1-F(x))^{n-r}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.6

\subsubsection{} %4.6.1

By CLT,
\begin{align}
    \sqrt{n}(\bar{X_n} - p)\xrightarrow{d} N(0,p(1-p)).
\end{align}

Let $g(x)=\sin^{-1}(\sqrt{x})$,
then $g'(p)=\frac{1}{2\sqrt{p(1-p)}}\ne 0$.
Then by Delta theorem
\begin{align}
    \sqrt{n}(\sin^{-1}\sqrt{\bar{X_n}} - \sin^{-1}\sqrt{p})\xrightarrow{d} N(0,1/4).
\end{align}

\subsubsection{} %4.6.2

\begin{align}
    S_n
        &= \frac{1}{n}\sum_{i=1}^n ((X_i-\mu) - (\bar X - \mu))^2 \\
        &= \frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - (\bar X - \mu)^2.
\end{align}

By Weak Law of Large Number, $\bar X - \mu\xrightarrow{p} 0$.
We have $\frac{(\bar X - \mu)^2}{\sqrt{n}}\xrightarrow{p} 0$ (easy to prove).

Since $\big(\frac{X_i-\mu}{\sigma}\big)^2\sim\chi^2$, for all $i$,
by CLT, we have
\begin{equation}
    \sqrt{n}\bigg(\frac{\sum_{i=1}^n \big(\frac{X_i-\mu}{\sigma}\big)^2}{n} - 1\bigg) \xrightarrow{d} N(0,2).
\end{equation}
So,
\begin{equation}
    \sqrt{n}\bigg(\frac{\sum_{i=1}^n (X_i-\mu)^2}{n} - \sigma^2\bigg) \xrightarrow{d} N(0,2\sigma^4).
\end{equation}

By Slutsky’s theorem,
\begin{equation}
    \sqrt{n}\big(S_n-\sigma^2\big) = \sqrt{n}\bigg(\frac{\sum_{i=1}^n (X_i-\mu)^2}{n} - \sigma^2\bigg) - \frac{(\bar X - \mu)^2}{\sqrt{n}}\xrightarrow{d} N(0,2\sigma^4).
\end{equation}

Let $g(x)=\log(x)$,
then $g'(\sigma^2)=\frac{1}{\sigma^2}\ne 0$.
Then by Delta theorem
\begin{equation}
    \sqrt{n}\big(\log(S_n)-\log(\sigma^2)\big) \xrightarrow{d} N(0,2).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.7

\begin{align}
    E(g'(x))
        &=\int_{-\infty}^{\infty} h(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)g'(x)dx \\
        &=-\int_{-\infty}^{\infty} \bigg(h(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)\bigg)'g(x)dx \\
        &=-\int_{-\infty}^{\infty} g(x)c(\vtheta)\exp\bigg(\sum_{i=1}^{k}w_i(\vtheta)T_i(x)\bigg)\bigg(h'(x)+h(x)\sum_{i=1}^k w_i(\vtheta)T_i'(x)\bigg)dx \\
        &=-E\bigg[g(x)\bigg(\frac{h'(x)}{h(x)}+\sum_{i=1}^k w_i(\vtheta)T_i'(x)\bigg)\bigg]
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.8

Let $T(\vx):=\sum_{i=1}^n X_i$, then
\begin{equation}
    \frac{P(\vx|\theta)}{P(T(\vx)|\theta)}
        = \frac{\prod_{i=1}^n \theta(1-\theta)^{x_i-1}}{\binom{T(\vx)-1}{n-1}\theta^n(1-\theta)^{T(\vx)-n}}
        = \frac{1}{\binom{T(\vx)-1}{n-1}}.
\end{equation}
So $\sum_{i=1}^n X_i$ is a sufficient statistic.

Let $g$ be a function. Then for $0<\theta<1$, let
\begin{align}
    0 = E_\theta(g(T))
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}\theta^n(1-\theta)^{t-n} \\
        &= \theta^n(1-\theta)^{-n}\sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}(1-\theta)^{t}.
\end{align}
Since $\theta^n(1-\theta)^{-n}\ne0$ when $0<\theta<1$, we have
\begin{align}
    0
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}(1-\theta)^{t} \\
        &= \sum_{t=n}^{\infty} g(t) \binom{t-1}{n-1}r^{t}.
\end{align}
for all $r$, where $0\le r<1$.
Then, $g(t)$ must be $0$ for all $t\ge n$.
If not, let $k$ be the smallest number such that $g(k)\ne 0$.
then we can rewrite as $0=r^k h(r)$, where $h(0)=\binom{k-1}{n-1}g(k)\ne 0$.
There exists an $\epsilon>0$, such that $h(r)\ne 0$ if $0\le r<\epsilon$.
This is a contradiction because $r^k h(r)=0$ for all $0\le r<1$.

We have $P_\theta(g(T)=0)=1$ for all $\theta$.
So, $\sum_{i=1}^n X_i$ is complete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.9

Let $n=\sum_{i=1}^4 x_i$. Denote $\vx=(\vx_1,\dots,\vx_N)$,
and $\vx_t=(x_{1,t}, \dots, x_{4,t})$.
\begin{align}
    f(\vx|\theta)
        &= \prod_{t=1}^N \frac{n!}{x_{1,t}!x_{2,t}!x_{3,t}!x_{4,t}!}
            \bigg(\frac{1}{2}+\frac{\theta}{4}\bigg)^{x_{1,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{x_{2,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{x_{3,t}}
            \bigg(\frac{\theta}{4}\bigg)^{n-\sum_{i=1}^3 x_{i,t}} \\
        &=  h(\vx)
            \bigg(\frac{1}{2}+\frac{\theta}{4}\bigg)^{\sum_{t=1}^N x_{1,t}}
            \bigg(\frac{1}{4}-\frac{\theta}{4}\bigg)^{\sum_{t=1}^N x_{2,t} + x_{3,t}}
            \bigg(\frac{\theta}{4}\bigg)^{nN-\sum_{i=1}^3\sum_{t=1}^N  x_{i,t}}.
\end{align}
By the factorization theorem,
$\big(\sum_{t=1}^N x_{1,t}, \sum_{t=1}^N x_{2,t} + x_{3,t}\big)$
is an sufficient statistic.

To show it is also minimal, we compute
\begin{align}
    \frac{f(\vx|\theta)}{f(\vy|\theta)}
        &= \frac{
            h(\vx)
            \big(\frac{1}{2}+\frac{\theta}{4}\big)^{\sum_{t=1}^N x_{1,t}}
            \big(\frac{1}{4}-\frac{\theta}{4}\big)^{\sum_{t=1}^N x_{2,t} + x_{3,t}}
            \big(\frac{\theta}{4}\big)^{nN-\sum_{i=1}^3\sum_{t=1}^N  x_{i,t}}
        }{
            h(\vy)
            \big(\frac{1}{2}+\frac{\theta}{4}\big)^{\sum_{t=1}^N y_{1,t}}
            \big(\frac{1}{4}-\frac{\theta}{4}\big)^{\sum_{t=1}^N y_{2,t} + y_{3,t}}
            \big(\frac{\theta}{4}\big)^{nN-\sum_{i=1}^3\sum_{t=1}^N  y_{i,t}}
        }.
\end{align}
It is independent of $\theta$ iff
$\sum_{t=1}^N x_{1,t} = \sum_{t=1}^N y_{1,t}$ and
$\sum_{t=1}^N x_{2,t} + x_{3,t} = \sum_{t=1}^N y_{2,t} + y_{3,t}$.
So, $\big(\sum_{t=1}^N x_{1,t}, \sum_{t=1}^N x_{2,t} + x_{3,t}\big)$
is also minimal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.10

\subsubsection{} %4.10.1

\begin{align}
    f(x,n|\theta)
        &= p_n \binom{n}{x} \theta^x(1-\theta)^{n-x}.
\end{align}
So,
\begin{align}
    \frac{f(x,n|\theta)}{f(y, m|\theta)}
    &= \frac{ p_n \binom{n}{x} \theta^x(1-\theta)^{n-x}}
        {p_m \binom{m}{y} \theta^y(1-\theta)^{m-y}} \\
    &= \frac{ p_n \binom{n}{x}} {p_m \binom{m}{y}}
        \theta^{x-y}(1-\theta)^{(n-m)-(x-y)}
\end{align}
It is independent of $\theta$ iff $x=y$ and $n=m$.
So $(X, N)$ is a minimal sufficient.

Since the distribution of $N$ does not depend on $\theta$,
by definition $N$ is ancillary for $\theta$.

\subsubsection{} %4.10.2

\begin{align}
    E\bigg[\frac{X}{N}\bigg]
        &= \sum_{n=1}^\infty \sum_{x=0}^n f(x,n) \frac{x}{n} \\
        &= \sum_{n=1}^\infty \sum_{x=0}^n p_n \binom{n}{x} \theta^x(1-\theta)^{n-x} \frac{x}{n} \\
        &= \theta \sum_{n=1}^\infty p_n \sum_{x=1}^n \binom{n-1}{x-1} \theta^{x-1}(1-\theta)^{n-x} \\
        &= \theta.
\end{align}
So $\frac{X}{N}$ is unbiased.
\begin{align}
    E\bigg[\bigg(\frac{X}{N}\bigg)^2\bigg]
        &= \sum_{n=1}^\infty \sum_{x=0}^n p_n \binom{n}{x} \theta^x(1-\theta)^{n-x} \bigg(\frac{x}{n}\bigg)^2 \\
        &= \theta \sum_{n=1}^\infty \frac{p_n}{n} \sum_{x=1}^n \binom{n-1}{x-1} \theta^{x-1}(1-\theta)^{n-x}[1+(x-1)] \\
        &= \theta \sum_{n=1}^\infty \frac{p_n}{n}
            + \theta \sum_{n=1}^\infty\frac{p_n(n-1)\theta}{n} \\
        &= (\theta - \theta^2) E\bigg[\frac{1}{N}\bigg] + \theta^2. \\
\end{align}
So $Var\big(\frac{X}{N}\big) = E\big[\big(\frac{X}{N}\big)^2\big] - \big(E\big[\frac{X}{N}\big]\big)^2=\theta(1-\theta)E\big[\frac{1}{N}\big]$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.11

First we need to show that $X_{(m+1)}$ is an unbiased estimator for $\mu$.
If $\mu=0$, then $E[X_{(m+1)}]$ must be $0$ because of mirror symmetry along $0$.
For general $\mu$, $E[X_{(m+1)}]=\mu$ by translation property.

Then we only need to show that $\bar{X}_{2m+1}$ is UMVUE.
This is easy to check because it attains the Cram\'er-Rao bound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.12

\subsubsection{} %4.12.1

First we show $T_1, T_2$ do not depend on $\mu$,
and thus $T_1/T_2$ is independent with $\mu$.
Use $T_1$ as an example.
Denote $F_\sigma$ as the cdf of $T_1$ when $\mu=0$.
\begin{align}
    P(T_1(\mX)\le t)
        &= P(T_1(\mX-\mu)\le t) = F_\sigma(t).
\end{align}
The right-hand side do not depend on $\mu$.

Then we show $T_1/T_2$ does not depend on $\sigma$.
Denote $F_\mu$ as the cdf of $T_1/T_2$ when $\sigma=1$.
then
\begin{align}
    P(T_1(\mX)/T_2(\mX)\le t)
        &= P(T_1(\sigma\mX)/T_2(\sigma\mX)\le t)
        = F_\mu (t).
\end{align}
The right-hand side do not depend on $\sigma$.

\subsubsection{} %4.12.2

Easy to check $R$ and $S$ satisfy the condition in previous sub-question.
Then $R/S$ is an ancillary statistic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.13

\subsubsection{} %4.13.1

For $0<p<1$, we must have
\begin{align}
    0
        &=E[U(X)] \\
        &= p U(-1) + \sum_{k\ge 0} (1-p)^2 p^k U(k) \\
        &= p U(-1) + \sum_{k\ge 0} (1-2p+p^2) p^k U(k) \\
        &= p U(-1) + \sum_{k\ge 0} p^k U(k) - 2 \sum_{k\ge 1} p^k U(k-1) + \sum_{k\ge 2} p^k U(k-2) \\
        &= U(0) + p\big(U(-1)+U(1)-2U(0)\big) + \sum_{p\ge2}p^k\big(U(k) -2U(k-1) +U(k-2)\big).
\end{align}
The coefficients before $p$ must be $0$. So,
\begin{align}
    U(0) &= 0, \\
    U(k) -2U(k-1) +U(k-2) &= 0, \quad k\ge 1.
\end{align}
From the relation, we can compute
\begin{align}
    U(1) &= -U(-1), \\
    U(2) &= -2U(-1), \\
    U(3) &= -3U(-1), \\
    \dots&\dots
\end{align}
Easy to prove by induction that $U(k)=-kU(-1)$.

\subsubsection{} %4.13.2

$\delta_0$ is an unbiased estimator of $p$.
\begin{align}
    Cov[\delta_0, U(X)] = E \delta_0 U(X) = U(-1).
\end{align}
So if $U(-1)\ne 0$, $\delta_0$ is not UMVUE because it correlates with
an unbiased estimator of $0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.14

$\sum_{i=1}^n X_i$ is a complete sufficient statistic.
Since
\begin{align}
    E \Big(\sum_{i=1}^n X_i\Big)^2
        &= n E X^2 + n(n-1) (E X)^2 \\
        &= np + n(n-1)p^2.
\end{align}
So
\begin{align}
    E \Bigg[\frac{n \sum_{i=1}^n X_i - \Big(\sum_{i=1}^n X_i\Big)^2 }{n(n-1)}\Bigg] = p(1-p).
\end{align}
And we get what we want.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.15

$\sum_{i=1}^n X_i = n\bar X$ is a complete sufficient statistic.
Define $T:=\sqrt{n}(\bar X - \mu) + \mu \sim N(\mu, 1)$.
We want a function $h(\bar X)$, \st
\begin{align}
    E[h(T)]
        &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty
            h(T) e^{\frac{(T-\mu)^2}{2}}dT \\
        &= P(X_1\le u) \\
        &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^u e^{\frac{(X_1-\mu)^2}{2}} d X_1
\end{align}
So, we know $h(T)=\doubleone(T\le u)=\doubleone(\sqrt{n}(\bar X - \mu) + \mu \le u)$
is the UMVUE of $\Phi(u-\mu)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{} %4.16
